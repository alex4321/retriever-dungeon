{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.12.5 in /opt/conda/lib/python3.7/site-packages (4.12.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2.25.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2021.10.21)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (21.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.1.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.5) (3.6.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (8.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "Requirement already satisfied: pandas==1.3.4 in /opt/conda/lib/python3.7/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
      "Requirement already satisfied: tqdm==4.62.3 in /opt/conda/lib/python3.7/site-packages (4.62.3)\n",
      "Requirement already satisfied: nltk==3.6.5 in /opt/conda/lib/python3.7/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (2021.10.21)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk==3.6.5) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.5) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.5) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.12.5\n",
    "!pip install pandas==1.3.4\n",
    "!pip install tqdm==4.62.3\n",
    "!pip install nltk==3.6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, GPT2TokenizerFast, BertModel, GPTNeoForCausalLM\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVER_BERT_MODEL = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "GENERATOR_GPTNEO_MODEL = \"EleutherAI/gpt-neo-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tokenizer = BertTokenizerFast.from_pretrained(RETRIEVER_BERT_MODEL)\n",
    "retriever_tokenizer.add_tokens([\"[STORY]\", \"[EXTRA]\", \"[RETRIEVE]\"])\n",
    "\n",
    "generator_tokenizer = GPT2TokenizerFast.from_pretrained(GENERATOR_GPTNEO_MODEL)\n",
    "generator_tokenizer.add_tokens([\"[TAGS]\", \"[INIT]\", \"[PROMPT]\", \"[TEXT]\", \"[INPUT]\", \"[OUTPUT]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'fit_denses.1.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.bias', 'cls.predictions.bias', 'fit_denses.2.weight', 'cls.seq_relationship.bias', 'fit_denses.1.weight', 'cls.seq_relationship.weight', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.3.weight', 'fit_denses.2.bias', 'fit_denses.0.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "retriever = BertModel.from_pretrained(RETRIEVER_BERT_MODEL)\n",
    "generator = GPTNeoForCausalLM.from_pretrained(GENERATOR_GPTNEO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_tokens(tokenizer, model):\n",
    "    if len(tokenizer) % 8 != 0:\n",
    "        tokenizer.add_tokens([\n",
    "            f\"[DUMB{i}]\"\n",
    "            for i in range(8 - len(tokenizer) % 8)\n",
    "        ])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tokenizer, retriever = sync_tokens(retriever_tokenizer, retriever)\n",
    "generator_tokenizer, generator = sync_tokens(generator_tokenizer, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_apply(df, func, chunk_size, process_count):\n",
    "    chunk_count = int(np.ceil(len(df) / chunk_size))\n",
    "    return list(chain(*Parallel(n_jobs=process_count)(\n",
    "        delayed(func)(df.iloc[i * chunk_size : (i + 1) * chunk_size])\n",
    "        for i in tqdm(range(chunk_count))\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_ids(input_ids):\n",
    "    return np.array(input_ids, dtype=np.int32).tobytes()\n",
    "\n",
    "def decode_input_ids(buffer):\n",
    "    return np.frombuffer(buffer, dtype=np.int32)\n",
    "\n",
    "def apply_tokenizer(texts, tokenizer):\n",
    "    return [\n",
    "        encode_input_ids(row)\n",
    "        for row in tokenizer(list(texts))[\"input_ids\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences_guttenberg = pd.read_csv(\"data/guttenberg-sentences-sampled.csv\")\n",
    "df_sentences_story = pd.read_csv(\"data/cleaned/story-sentences.csv\")\n",
    "df_sentences_context_mapping = pd.read_csv(\"data/cleaned/story-context-sentence-mapping-numeric-id.csv\")\n",
    "df_story_content = pd.read_csv(\"data/cleaned/story-trees-numeric-id.csv\")\n",
    "df_stories_train = pd.read_csv(\"data/cleaned/stories-train.csv\")\n",
    "df_stories_test = pd.read_csv(\"data/cleaned/stories-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>story_id</th>\n",
       "      <th>children_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>The land of Kronnland is a mythical, wonderful...</td>\n",
       "      <td>12487</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Start Danny's Campaign</td>\n",
       "      <td>Danny Blaze\\nBackground :\\nBorn in the summer ...</td>\n",
       "      <td>12487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Continue</td>\n",
       "      <td>With all the townsfolk transformed into mindle...</td>\n",
       "      <td>12487</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Get back to Bren and warn him about the danger.</td>\n",
       "      <td>You run down the hill as Andrew's army regroup...</td>\n",
       "      <td>12487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Watch the battle from your hideout.</td>\n",
       "      <td>Although worried, you stay in your hideout and...</td>\n",
       "      <td>12487</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  parent_id                                            input  \\\n",
       "0   0         -1                                           [ROOT]   \n",
       "1   1          0                          Start Danny's Campaign    \n",
       "2   2          1                                         Continue   \n",
       "3   3          2  Get back to Bren and warn him about the danger.   \n",
       "4   4          2              Watch the battle from your hideout.   \n",
       "\n",
       "                                              output  story_id  children_count  \n",
       "0  The land of Kronnland is a mythical, wonderful...     12487               3  \n",
       "1  Danny Blaze\\nBackground :\\nBorn in the summer ...     12487               1  \n",
       "2  With all the townsfolk transformed into mindle...     12487               2  \n",
       "3  You run down the hill as Andrew's army regroup...     12487               0  \n",
       "4  Although worried, you stay in your hideout and...     12487               2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2children_count = df_story_content.groupby([\"story_id\", \"parent_id\"])[\"id\"].nunique().to_dict()\n",
    "df_story_content[\"children_count\"] = df_story_content[[\"story_id\", \"id\"]].apply(\n",
    "    lambda row: id2children_count.get((row[\"story_id\"], row[\"id\"]), 0),\n",
    "    axis=1\n",
    ")\n",
    "df_story_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:28<00:00,  6.55it/s]\n",
      " 59%|█████▊    | 108/184 [00:16<00:13,  5.45it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2103 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 184/184 [00:30<00:00,  6.05it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sentences_guttenberg[\"retriever_input_ids\"] = parallel_apply(\n",
    "    df_sentences_guttenberg,\n",
    "    lambda df: apply_tokenizer(\"[EXTRA] \" + df[\"text\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_sentences_guttenberg[\"generator_input_ids\"] = parallel_apply(\n",
    "    df_sentences_guttenberg,\n",
    "    lambda df: apply_tokenizer(\"[PROMPT] \" + df[\"text\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:04<00:00, 15.67it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.50it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sentences_story[\"retriever_input_ids\"] = parallel_apply(\n",
    "    df_sentences_story,\n",
    "    lambda df: apply_tokenizer(\"[STORY] \" + df[\"text\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_sentences_story[\"generator_input_ids\"] = parallel_apply(\n",
    "    df_sentences_story,\n",
    "    lambda df: apply_tokenizer(\"[PROMPT] \" + df[\"text\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 5315.97it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 5924.16it/s]\n"
     ]
    }
   ],
   "source": [
    "df_story_content[\"input_retriever_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[RETRIEVE] \" + df[\"input\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_story_content[\"input_generator_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[INPUT] \" + df[\"input\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 6036.21it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2157 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2219 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2105 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2362 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2056 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2088 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3423 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 7/7 [00:00<00:00, 5346.95it/s]\n"
     ]
    }
   ],
   "source": [
    "df_story_content[\"output_generator_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[OUTPUT] \" + df[\"output\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_story_content[\"output_retriever_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[RETRIEVE] \" + df[\"output\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>text</th>\n",
       "      <th>retriever_input_ids</th>\n",
       "      <th>generator_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Gobryas is there?\"</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x00&amp;\\x00\\x00\\x00\\xa0\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>His name's Gonzago.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00_\\t\\x00\\x00\\x9e\\x05\\x00\\x00R\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Goneril, gonəril.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x9b8\\x00\\x00Z\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>In discussing the character of Hlestakov, the ...</td>\n",
       "      <td>b\"e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00*\\x02\\x00\\x00\\x86+\\x00\\x00\\x06\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Gomalco Productions.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x80F\\x00\\x001\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                               text  \\\n",
       "0        0                                \"Gobryas is there?\"   \n",
       "1        0                                His name's Gonzago.   \n",
       "2        0                                  Goneril, gonəril.   \n",
       "3        0  In discussing the character of Hlestakov, the ...   \n",
       "4        0                               Gomalco Productions.   \n",
       "\n",
       "                                 retriever_input_ids  \\\n",
       "0  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "1  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "2  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "3  b\"e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "4  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "\n",
       "                                 generator_input_ids  \n",
       "0  b'S\\xc4\\x00\\x00n\\x01\\x00\\x00&\\x00\\x00\\x00\\xa0\\...  \n",
       "1  b'S\\xc4\\x00\\x00_\\t\\x00\\x00\\x9e\\x05\\x00\\x00R\\x0...  \n",
       "2  b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x9b8\\x00\\x00Z\\...  \n",
       "3  b'S\\xc4\\x00\\x00*\\x02\\x00\\x00\\x86+\\x00\\x00\\x06\\...  \n",
       "4  b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x80F\\x00\\x001\\...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_guttenberg = df_sentences_guttenberg.loc[\n",
    "    (df_sentences_guttenberg[\"retriever_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_guttenberg = df_sentences_guttenberg.loc[\n",
    "    (df_sentences_guttenberg[\"generator_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_guttenberg = df_sentences_guttenberg.reset_index(drop=True)\n",
    "df_sentences_guttenberg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retriever_input_ids</th>\n",
       "      <th>generator_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Sorry, Soren.\"</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x0018\\x00\\x00\\x0b\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3421</td>\n",
       "      <td>Are they alive?</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x87\\x10\\x00\\x00\\xe4\\x01\\x00\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3420</td>\n",
       "      <td>What DO you do?</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00K\\x07\\x00\\x00\\xda \\x00\\x00Y\\x01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3419</td>\n",
       "      <td>\"This is yours.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x00\\xbc\\x04\\x00\\x00&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3418</td>\n",
       "      <td>Leave the halls</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00&amp;D\\x00\\x00\\x06\\x01\\x00\\x00\\x1e_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id             text                                retriever_input_ids  \\\n",
       "0     0  \"Sorry, Soren.\"  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "1  3421  Are they alive?  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "2  3420  What DO you do?  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "3  3419  \"This is yours.  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "4  3418  Leave the halls  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "\n",
       "                                 generator_input_ids  \n",
       "0  b'S\\xc4\\x00\\x00n\\x01\\x00\\x0018\\x00\\x00\\x0b\\x00...  \n",
       "1  b'S\\xc4\\x00\\x00\\x87\\x10\\x00\\x00\\xe4\\x01\\x00\\x0...  \n",
       "2  b'S\\xc4\\x00\\x00K\\x07\\x00\\x00\\xda \\x00\\x00Y\\x01...  \n",
       "3  b'S\\xc4\\x00\\x00n\\x01\\x00\\x00\\xbc\\x04\\x00\\x00>\\...  \n",
       "4  b'S\\xc4\\x00\\x00&D\\x00\\x00\\x06\\x01\\x00\\x00\\x1e_...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_story = df_sentences_story.loc[\n",
    "    (df_sentences_story[\"retriever_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_story = df_sentences_story.loc[\n",
    "    (df_sentences_story[\"generator_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_story = df_sentences_story.reset_index(drop=True)\n",
    "df_sentences_story.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import chain\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-4\n",
    "RETRIEVER_INPUT_MAX_LENGTH = 256\n",
    "RETRIEVER_LAST_OUTPUT_SENTENCES = 3\n",
    "KNN_N_NEIGHBOURS = 4\n",
    "MAX_RELEVANT_TOKENS = 512\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(retriever, retriever_tokenizer, input_ids):\n",
    "    def _prepare_ids(ids, max_length):\n",
    "        ids = list(ids)\n",
    "        if len(ids) < max_length:\n",
    "            return ids + [retriever_tokenizer.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            return ids[:max_length]\n",
    "\n",
    "    max_length = max([len(item) for item in input_ids])\n",
    "    if max_length % 8 != 0:\n",
    "        max_length += 8 - max_length % 8\n",
    "    if max_length > RETRIEVER_INPUT_MAX_LENGTH:\n",
    "        max_length = RETRIEVER_INPUT_MAX_LENGTH\n",
    "    \n",
    "    padded_input_ids = torch.LongTensor([\n",
    "        _prepare_ids(item, max_length) for item in input_ids\n",
    "    ])\n",
    "    attention_mask = padded_input_ids != retriever_tokenizer.pad_token_id\n",
    "    hidden_state = retriever(input_ids=padded_input_ids.to(retriever.device),\n",
    "                             attention_mask=attention_mask.to(retriever.device),\n",
    "                             output_hidden_states=True).last_hidden_state\n",
    "    cls_embedding = hidden_state[:, 0, :]\n",
    "    cls_embedding_norm = torch.sqrt( (cls_embedding ** 2).sum(dim=-1, keepdims=True) ) + EPS\n",
    "    return cls_embedding / cls_embedding_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(retriever, retriever_tokenizer, inputs, batch_size, verbose=False):\n",
    "    inputs = list(inputs)\n",
    "    df_sort = pd.DataFrame({\n",
    "        \"index\": range(len(inputs)),\n",
    "        \"inputs\": inputs,\n",
    "        \"length\": [len(row) for row in inputs]\n",
    "    })\n",
    "    df_sort = df_sort.sort_values(\"length\", ascending=False)\n",
    "\n",
    "    embeddings = np.zeros([len(inputs), retriever.config.hidden_size], dtype=np.float16)\n",
    "\n",
    "    batch_count = int(np.ceil(len(inputs) / batch_size))\n",
    "    with torch.no_grad():\n",
    "        iterable = range(batch_count)\n",
    "        if verbose:\n",
    "            iterable = tqdm(iterable)\n",
    "        for i in iterable:\n",
    "            batch_df_sort = df_sort.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "            batch_input_ids = batch_df_sort[\"inputs\"].apply(decode_input_ids).tolist()\n",
    "            batch_embeddings_torch = get_batch_embeddings(retriever, retriever_tokenizer, batch_input_ids)\n",
    "            embeddings[batch_df_sort[\"index\"].tolist()] = batch_embeddings_torch.detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_prompt_embeddings(retriever, generator, retriever_tokenizer, df_sentences_guttenberg, df_sentences_story):\n",
    "    print(\"UPDATING EMBEDDINGS CACHE\")\n",
    "    retriever.eval()\n",
    "    retriever.cuda()\n",
    "    extra_embeddings = get_embeddings(retriever, retriever_tokenizer, df_sentences_guttenberg[\"retriever_input_ids\"], 64, verbose=True)\n",
    "    story_embeddings = get_embeddings(retriever, retriever_tokenizer, df_sentences_story[\"retriever_input_ids\"], 64, verbose=True)\n",
    "    return extra_embeddings, story_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrieverInput:\n",
    "    input_ids: np.ndarray\n",
    "    story_sentences: np.ndarray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StoryInputSample:\n",
    "    generator_input_ids: np.ndarray\n",
    "    generator_input_weights: np.ndarray\n",
    "    retriever_inputs: List[RetrieverInput]\n",
    "\n",
    "\n",
    "def get_rows(df_story_content, id, story_id):\n",
    "    rows = []\n",
    "    df_story_content = df_story_content.loc[df_story_content[\"story_id\"] == story_id].set_index(\"id\")\n",
    "    while id != -1:\n",
    "        try:\n",
    "            row = df_story_content.loc[[id]].iloc[0]\n",
    "        except:\n",
    "            break\n",
    "        rows.append(row)\n",
    "        id = row[\"parent_id\"]\n",
    "    return rows[::-1]\n",
    "\n",
    "\n",
    "def extract_story_inputs(rows, df_sentences_context_mapping):\n",
    "    def _get_generator_inputs(row):\n",
    "        if row[\"parent_id\"] != -1:\n",
    "            row_input = list(decode_input_ids(row[\"input_generator_input_ids\"]))\n",
    "        else:\n",
    "            row_input = []\n",
    "        row_output = list(decode_input_ids(row[\"output_generator_input_ids\"]))\n",
    "        row_content = row_input + row_output\n",
    "\n",
    "        return row_content\n",
    "\n",
    "    def _get_previous_sentences(row):\n",
    "        mask = (df_sentences_context_mapping[\"story_id\"] == row[\"story_id\"]) & \\\n",
    "               (df_sentences_context_mapping[\"context_id\"] == row[\"parent_id\"])\n",
    "        return np.array(sorted(df_sentences_context_mapping.loc[mask, \"sentence_id\"]))\n",
    "\n",
    "    def _get_retriever_input_ids(rows):\n",
    "        last_row = rows[-1]\n",
    "        input_sentences_pairs = []\n",
    "        for row in rows[-2:]:\n",
    "            if row[\"parent_id\"] != -1:\n",
    "                input_ids = decode_input_ids(row[\"input_retriever_input_ids\"])\n",
    "                story_sentence_ids = _get_previous_sentences(row)\n",
    "                input_sentences_pairs.append(RetrieverInput(input_ids, story_sentence_ids))\n",
    "        return input_sentences_pairs\n",
    "\n",
    "    generator_input_ids = []\n",
    "    generator_children_counts = []\n",
    "    for row in rows:\n",
    "        row_content = _get_generator_inputs(row)\n",
    "        if row[\"children_count\"] == 0:\n",
    "            generator_children_counts.append((len(row_content), 1))\n",
    "        else:\n",
    "            generator_children_counts.append((len(row_content), row[\"children_count\"]))\n",
    "        generator_input_ids += row_content\n",
    "    generator_weights = []\n",
    "    k = 1.0\n",
    "    for token_count, children_count in generator_children_counts[::-1]:\n",
    "        k *= (1 / children_count)\n",
    "        generator_weights += [k] * token_count\n",
    "    generator_weights = generator_weights[::-1]\n",
    "    retriever_inputs = _get_retriever_input_ids(rows)\n",
    "    return StoryInputSample(generator_input_ids, generator_weights, retriever_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_description_encode(df_stories, story_id, generator_tokenizer):\n",
    "    tags = \"[TAGS] \" + df_stories.loc[df_stories[\"id\"] == story_id, \"tags\"].values[0]\n",
    "    return generator_tokenizer.encode(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_input(df_story_content, row_id, story_id, df_sentences_context_mapping, retriever_tokenizer):\n",
    "    rows = get_rows(df_story_content, row_id, story_id)\n",
    "    if len(rows) == 1:\n",
    "        parent_id = rows[-1][\"parent_id\"]\n",
    "    else:\n",
    "        parent_id = rows[-2][\"parent_id\"]\n",
    "    retriever_requests = [\n",
    "        (\n",
    "            decode_input_ids(rows[-1][\"input_retriever_input_ids\"]).tolist(),\n",
    "            df_sentences_context_mapping.loc[\n",
    "                (df_sentences_context_mapping[\"story_id\"] == story_id) & \\\n",
    "                (df_sentences_context_mapping[\"context_id\"] == parent_id),\n",
    "                \"sentence_id\"\n",
    "            ].tolist()\n",
    "        )\n",
    "    ]\n",
    "    if len(rows) > 1:\n",
    "        sentences_to_search = df_sentences_context_mapping.loc[\n",
    "            (df_sentences_context_mapping[\"story_id\"] == story_id) & \\\n",
    "            (df_sentences_context_mapping[\"context_id\"] == parent_id),\n",
    "            \"sentence_id\"\n",
    "        ].tolist()\n",
    "        query_sentences = nltk.sent_tokenize(rows[-2][\"output\"])[-RETRIEVER_LAST_OUTPUT_SENTENCES:]\n",
    "        retriever_requests += [\n",
    "            (retriever_tokenizer.encode(sent), sentences_to_search)\n",
    "            for sent in query_sentences\n",
    "        ]\n",
    "    return extract_story_inputs(rows, df_sentences_context_mapping), retriever_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_distance(x, y):\n",
    "    return -(x * y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_nearest_df(retriever_request_embeddings, cached_extra_nn, cached_story_nn, df_extra, df_story):\n",
    "    retriever_request_embeddings_np = retriever_request_embeddings.detach().cpu().numpy()\n",
    "    extra_indices = []\n",
    "    extra_distances = []\n",
    "    for distances, indices in zip(*cached_extra_nn.kneighbors(retriever_request_embeddings_np)):\n",
    "        extra_indices += list(indices)\n",
    "        extra_distances += list(distances)\n",
    "    sub_df_extra = df_extra.iloc[extra_indices][[\"text\", \"retriever_input_ids\", \"generator_input_ids\"]]\n",
    "    sub_df_extra[\"distance\"] = extra_distances\n",
    "\n",
    "    if cached_story_nn is not None:\n",
    "        story_indices = []\n",
    "        story_distances = []\n",
    "        for distances, indices in zip(*cached_story_nn.kneighbors(retriever_request_embeddings_np)):\n",
    "            story_indices += list(indices)\n",
    "            story_distances += list(distances)\n",
    "        sub_df_story = df_story.iloc[story_indices][[\"text\", \"retriever_input_ids\", \"generator_input_ids\"]]\n",
    "        sub_df_story[\"distance\"] = story_distances\n",
    "    \n",
    "    if cached_story_nn is not None:\n",
    "        df = pd.concat([sub_df_extra, sub_df_story]).reset_index(drop=True)\n",
    "    else:\n",
    "        df = sub_df_extra.reset_index(drop=True)\n",
    "    df = df.sort_values(\"distance\")\n",
    "    df = df.drop_duplicates(\"text\")\n",
    "    df = df.head( (RETRIEVER_LAST_OUTPUT_SENTENCES + 1) * KNN_N_NEIGHBOURS)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_generator_input(input_ids, weights, generator, generator_tokenizer, tag_input_ids, nearest_input_ids):\n",
    "    max_story_token_count = generator.config.max_position_embeddings - len(tag_input_ids) - len(nearest_input_ids)\n",
    "    generator_input_ids = input_ids[-max_story_token_count:]\n",
    "    generator_weights = weights[-max_story_token_count:]\n",
    "\n",
    "    input_tid, = generator_tokenizer.convert_tokens_to_ids([\"[INPUT]\"])\n",
    "    output_tid, = generator_tokenizer.convert_tokens_to_ids([\"[OUTPUT]\"])\n",
    "\n",
    "    if generator_input_ids[0] not in {input_tid, output_tid}:\n",
    "        if input_tid not in generator_input_ids:\n",
    "            input_start = None\n",
    "        else:\n",
    "            input_start = list(generator_input_ids).index(input_tid)\n",
    "        if output_tid not in generator_input_ids:\n",
    "            output_start = None\n",
    "        else:\n",
    "            output_start = list(generator_input_ids).index(output_tid)\n",
    "        if output_start is not None and input_start is not None:\n",
    "            if input_start < output_start:\n",
    "                start_token = output_tid\n",
    "            else:\n",
    "                start_token = input_tid\n",
    "        elif output_start is not None:\n",
    "            start_token = input_tid\n",
    "        else:\n",
    "            start_token = output_tid\n",
    "        generator_input_ids[0] = start_token\n",
    "\n",
    "    return generator_input_ids, generator_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_input(cached_extra_nn, row, df_story_content, df_stories):\n",
    "    generator_input, retriever_requests = story_input(df_story_content,\n",
    "                                                      row[\"id\"],\n",
    "                                                      row[\"story_id\"],\n",
    "                                                      df_sentences_context_mapping,\n",
    "                                                      retriever_tokenizer)\n",
    "    story_sentence_ids = set(chain(*[sentences for _, sentences in retriever_requests]))\n",
    "    story_sentence_mask = df_sentences_story[\"id\"].isin(story_sentence_ids)\n",
    "    story_sentence_count = story_sentence_mask.sum()\n",
    "\n",
    "    if story_sentence_count > 0:\n",
    "        cached_story_nn = NearestNeighbors(n_neighbors=min(KNN_N_NEIGHBOURS, int(story_sentence_mask.sum())),\n",
    "                                        metric=dot_distance,\n",
    "                                        n_jobs=-1,\n",
    "                                        algorithm=\"brute\")\n",
    "        cached_story_nn.fit(cached_story_embeddings[story_sentence_mask])\n",
    "    else:\n",
    "        cached_story_nn = None\n",
    "\n",
    "    retriever_request_embeddings = get_batch_embeddings(retriever, retriever_tokenizer, [\n",
    "        input_ids\n",
    "        for input_ids, _ in retriever_requests\n",
    "    ])\n",
    "    df_cached_nearest = get_cached_nearest_df(retriever_request_embeddings,\n",
    "                                              cached_extra_nn,\n",
    "                                              cached_story_nn,\n",
    "                                              df_sentences_guttenberg,\n",
    "                                              df_sentences_story.loc[story_sentence_mask])\n",
    "    retriever_cached_relevant_embeddings = get_batch_embeddings(\n",
    "        retriever,\n",
    "        retriever_tokenizer,\n",
    "        df_cached_nearest[\"retriever_input_ids\"].apply(decode_input_ids)\n",
    "    )\n",
    "    retriever_distances = -retriever_request_embeddings.matmul(retriever_cached_relevant_embeddings.T)\n",
    "\n",
    "    retriever_nearest_indices = retriever_distances.mean(dim=0).sort().indices.detach().cpu().numpy()\n",
    "    df_nearest = df_cached_nearest.iloc[retriever_nearest_indices]\n",
    "    nearest_samples_input_ids = df_nearest[\"generator_input_ids\"].apply(decode_input_ids)    \n",
    "\n",
    "    nearest_input_ids = np.array(list(chain(*nearest_samples_input_ids))[:MAX_RELEVANT_TOKENS-1])\n",
    "    nearest_weights = np.zeros([len(nearest_input_ids)])\n",
    "    \n",
    "    tags_string = df_stories.loc[df_stories[\"id\"] == row[\"story_id\"], \"tags\"].values[0]\n",
    "    if pd.isna(tags_string):\n",
    "        tags_string = \"\"\n",
    "    tag_input_ids = np.array(generator_tokenizer.encode(\"[TAGS] \" + tags_string))\n",
    "    tag_weights = np.zeros([len(tag_input_ids)])\n",
    "\n",
    "    generator_input_ids, generator_weights = cut_generator_input(\n",
    "        generator_input.generator_input_ids,\n",
    "        generator_input.generator_input_weights,\n",
    "        generator,\n",
    "        generator_tokenizer,\n",
    "        tag_input_ids,\n",
    "        nearest_input_ids\n",
    "    )\n",
    "\n",
    "    input_ids = list(tag_input_ids) + list(nearest_input_ids) + list(generator_input_ids)\n",
    "    weights = list(tag_weights) + list(nearest_weights) + list(generator_weights)\n",
    "\n",
    "    return input_ids, weights, retriever_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(input_ids, weights, logits):\n",
    "    batch_size, _, class_count = logits.shape\n",
    "    input_ids_shifted = input_ids[:, 1:].reshape([-1])\n",
    "    logits_shifted = logits[:, :-1, :].reshape([-1, class_count])\n",
    "    tokenwise_ce = F.cross_entropy(logits_shifted,\n",
    "                                   input_ids_shifted,\n",
    "                                   reduction=\"none\")\\\n",
    "        .reshape([batch_size, -1])\n",
    "    loss_samplewise = (tokenwise_ce * weights[:, 1:]).sum(dim=-1) / (weights[:, 1:].sum(dim=-1) + EPS)\n",
    "    return loss_samplewise.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_loss(cached_extra_nn, row, df_story_content_train, df_stories_train):\n",
    "    input_ids, weights, retriever_distances = get_nn_input(cached_extra_nn,\n",
    "                                                           row,\n",
    "                                                           df_story_content_train,\n",
    "                                                           df_stories_train)\n",
    "    generator_input_ids = torch.LongTensor([input_ids]).to(generator.device)\n",
    "    generator_weights = torch.FloatTensor([weights]).to(generator.device)\n",
    "    generator_output = generator(generator_input_ids).logits\n",
    "    loss = calc_loss(generator_input_ids, generator_weights, generator_output)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retriever.cuda()\n",
    "generator = generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_story_content_train = df_story_content.loc[df_story_content[\"story_id\"].isin(df_stories_train[\"id\"])]\n",
    "df_story_content_train = df_story_content_train.sample(len(df_story_content_train), random_state=42)\\\n",
    "    .reset_index(drop=True)\n",
    "    \n",
    "df_story_content_test = df_story_content.loc[df_story_content[\"story_id\"].isin(df_stories_test[\"id\"])]\n",
    "df_story_content_test = df_story_content_test.sample(len(df_story_content_test), random_state=42)\\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizable_params = list(generator.parameters()) + list(retriever.parameters())\n",
    "optimizer = Adam(optimizable_params, lr=LR)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [02:43<00:00, 161.31it/s]\n",
      "100%|██████████| 9749/9749 [00:59<00:00, 164.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17.497169494628906\n",
      "1 15.879802703857422\n",
      "2 14.699311256408691\n",
      "3 16.381174087524414\n",
      "4 15.230399131774902\n",
      "5 15.891133308410645\n",
      "6 16.060945510864258\n",
      "7 15.771090507507324\n",
      "8 14.704065322875977\n",
      "9 14.49145221710205\n",
      "10 17.262022018432617\n",
      "11 8.21269416809082\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 39.59 GiB total capacity; 37.66 GiB already allocated; 20.94 MiB free; 37.88 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15918/3824304401.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_story_content_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_row_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_extra_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_story_content_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stories_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15918/2597966313.py\u001b[0m in \u001b[0;36mget_row_loss\u001b[0;34m(cached_extra_nn, row, df_story_content_train, df_stories_train)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgenerator_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgenerator_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    628\u001b[0m                     \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m                 )\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 39.59 GiB total capacity; 37.66 GiB already allocated; 20.94 MiB free; 37.88 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "i = -1\n",
    "while True:\n",
    "    i += 1\n",
    "    if i == len(df_story_content_train):\n",
    "        i = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        if i % 500 == 0:\n",
    "            cached_extra_embeddings, cached_story_embeddings = get_cached_prompt_embeddings(retriever,\n",
    "                                                                                            generator,\n",
    "                                                                                            retriever_tokenizer,\n",
    "                                                                                            df_sentences_guttenberg,\n",
    "                                                                                            df_sentences_story)\n",
    "            cached_extra_nn = NearestNeighbors(n_neighbors=KNN_N_NEIGHBOURS, metric=dot_distance, n_jobs=-1, algorithm=\"ball_tree\")\n",
    "            cached_extra_nn.fit(cached_extra_embeddings)\n",
    "            retriever.train()\n",
    "            generator.train()\n",
    "        row = df_story_content_train.iloc[i]\n",
    "        loss = get_row_loss(cached_extra_nn, row, df_story_content_train, df_stories_train)    \n",
    "    print(i, loss.item())\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_ids.shape"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "interpreter": {
   "hash": "14491e825d4673210f59c83732ae5f4b564c5c040b5b1d881a577d4827971b6d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
