{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.12.5 in /opt/conda/lib/python3.7/site-packages (4.12.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (21.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (3.4.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2021.10.21)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.5) (3.6.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Requirement already satisfied: pandas==1.3.4 in /opt/conda/lib/python3.7/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
      "Requirement already satisfied: tqdm==4.62.3 in /opt/conda/lib/python3.7/site-packages (4.62.3)\n",
      "Requirement already satisfied: nltk==3.6.5 in /opt/conda/lib/python3.7/site-packages (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (2021.10.21)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (4.62.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk==3.6.5) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.5) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.5) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.12.5\n",
    "!pip install pandas==1.3.4\n",
    "!pip install tqdm==4.62.3\n",
    "!pip install nltk==3.6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, GPT2TokenizerFast, BertModel, GPTNeoForCausalLM\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVER_BERT_MODEL = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "GENERATOR_GPTNEO_MODEL = \"EleutherAI/gpt-neo-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tokenizer = BertTokenizerFast.from_pretrained(RETRIEVER_BERT_MODEL)\n",
    "retriever_tokenizer.add_tokens([\"[STORY]\", \"[EXTRA]\", \"[RETRIEVE]\"])\n",
    "\n",
    "generator_tokenizer = GPT2TokenizerFast.from_pretrained(GENERATOR_GPTNEO_MODEL)\n",
    "generator_tokenizer.add_tokens([\"[TAGS]\", \"[INIT]\", \"[PROMPT]\", \"[TEXT]\", \"[INPUT]\", \"[OUTPUT]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['fit_denses.4.bias', 'fit_denses.1.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'fit_denses.3.weight', 'fit_denses.1.weight', 'fit_denses.2.weight', 'fit_denses.0.weight', 'fit_denses.2.bias', 'fit_denses.3.bias', 'fit_denses.4.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "retriever = BertModel.from_pretrained(RETRIEVER_BERT_MODEL)\n",
    "generator = GPTNeoForCausalLM.from_pretrained(GENERATOR_GPTNEO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_tokens(tokenizer, model):\n",
    "    if len(tokenizer) % 8 != 0:\n",
    "        tokenizer.add_tokens([\n",
    "            f\"[DUMB{i}]\"\n",
    "            for i in range(8 - len(tokenizer) % 8)\n",
    "        ])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tokenizer, retriever = sync_tokens(retriever_tokenizer, retriever)\n",
    "generator_tokenizer, generator = sync_tokens(generator_tokenizer, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_apply(df, func, chunk_size, process_count):\n",
    "    chunk_count = int(np.ceil(len(df) / chunk_size))\n",
    "    return list(chain(*Parallel(n_jobs=process_count)(\n",
    "        delayed(func)(df.iloc[i * chunk_size : (i + 1) * chunk_size])\n",
    "        for i in tqdm(range(chunk_count))\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_ids(input_ids):\n",
    "    return np.array(input_ids, dtype=np.int32).tobytes()\n",
    "\n",
    "def decode_input_ids(buffer):\n",
    "    return np.frombuffer(buffer, dtype=np.int32)\n",
    "\n",
    "def apply_tokenizer(texts, tokenizer):\n",
    "    return [\n",
    "        encode_input_ids(row)\n",
    "        for row in tokenizer(list(texts))[\"input_ids\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences_guttenberg = pd.read_csv(\"data/guttenberg-sentences-sampled.csv\")\n",
    "df_sentences_story = pd.read_csv(\"data/cleaned/story-sentences.csv\")\n",
    "df_sentences_context_mapping = pd.read_csv(\"data/cleaned/story-context-sentence-mapping-numeric-id.csv\")\n",
    "df_story_content = pd.read_csv(\"data/cleaned/story-trees-numeric-id.csv\")\n",
    "df_stories_train = pd.read_csv(\"data/cleaned/stories-train.csv\")\n",
    "df_stories_test = pd.read_csv(\"data/cleaned/stories-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>story_id</th>\n",
       "      <th>children_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>The land of Kronnland is a mythical, wonderful...</td>\n",
       "      <td>12487</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Start Danny's Campaign</td>\n",
       "      <td>Danny Blaze\\nBackground :\\nBorn in the summer ...</td>\n",
       "      <td>12487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Continue</td>\n",
       "      <td>With all the townsfolk transformed into mindle...</td>\n",
       "      <td>12487</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Get back to Bren and warn him about the danger.</td>\n",
       "      <td>You run down the hill as Andrew's army regroup...</td>\n",
       "      <td>12487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Watch the battle from your hideout.</td>\n",
       "      <td>Although worried, you stay in your hideout and...</td>\n",
       "      <td>12487</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  parent_id                                            input  \\\n",
       "0   0         -1                                           [ROOT]   \n",
       "1   1          0                          Start Danny's Campaign    \n",
       "2   2          1                                         Continue   \n",
       "3   3          2  Get back to Bren and warn him about the danger.   \n",
       "4   4          2              Watch the battle from your hideout.   \n",
       "\n",
       "                                              output  story_id  children_count  \n",
       "0  The land of Kronnland is a mythical, wonderful...     12487               3  \n",
       "1  Danny Blaze\\nBackground :\\nBorn in the summer ...     12487               1  \n",
       "2  With all the townsfolk transformed into mindle...     12487               2  \n",
       "3  You run down the hill as Andrew's army regroup...     12487               0  \n",
       "4  Although worried, you stay in your hideout and...     12487               2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2children_count = df_story_content.groupby([\"story_id\", \"parent_id\"])[\"id\"].nunique().to_dict()\n",
    "df_story_content[\"children_count\"] = df_story_content[[\"story_id\", \"id\"]].apply(\n",
    "    lambda row: id2children_count.get((row[\"story_id\"], row[\"id\"]), 0),\n",
    "    axis=1\n",
    ")\n",
    "df_story_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:28<00:00,  6.54it/s]\n",
      " 59%|█████▊    | 108/184 [00:17<00:14,  5.40it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2103 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 184/184 [00:30<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sentences_guttenberg[\"retriever_input_ids\"] = parallel_apply(\n",
    "    df_sentences_guttenberg,\n",
    "    lambda df: apply_tokenizer(\"[EXTRA] \" + df[\"text\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_sentences_guttenberg[\"generator_input_ids\"] = parallel_apply(\n",
    "    df_sentences_guttenberg,\n",
    "    lambda df: apply_tokenizer(\"[PROMPT] \" + df[\"text\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:04<00:00, 15.04it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.53it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sentences_story[\"retriever_input_ids\"] = parallel_apply(\n",
    "    df_sentences_story,\n",
    "    lambda df: apply_tokenizer(\"[STORY] \" + df[\"text\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_sentences_story[\"generator_input_ids\"] = parallel_apply(\n",
    "    df_sentences_story,\n",
    "    lambda df: apply_tokenizer(\"[PROMPT] \" + df[\"text\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 6843.85it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 5956.61it/s]\n"
     ]
    }
   ],
   "source": [
    "df_story_content[\"input_retriever_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[RETRIEVE] \" + df[\"input\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_story_content[\"input_generator_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[INPUT] \" + df[\"input\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 5920.57it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2157 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2219 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2056 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2362 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2105 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3423 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2088 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 7/7 [00:00<00:00, 5673.45it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "df_story_content[\"output_generator_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[OUTPUT] \" + df[\"output\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_story_content[\"output_retriever_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[RETRIEVE] \" + df[\"output\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>text</th>\n",
       "      <th>retriever_input_ids</th>\n",
       "      <th>generator_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Gobryas is there?\"</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x00&amp;\\x00\\x00\\x00\\xa0\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>His name's Gonzago.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00_\\t\\x00\\x00\\x9e\\x05\\x00\\x00R\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Goneril, gonəril.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x9b8\\x00\\x00Z\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>In discussing the character of Hlestakov, the ...</td>\n",
       "      <td>b\"e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00*\\x02\\x00\\x00\\x86+\\x00\\x00\\x06\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Gomalco Productions.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x80F\\x00\\x001\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                               text  \\\n",
       "0        0                                \"Gobryas is there?\"   \n",
       "1        0                                His name's Gonzago.   \n",
       "2        0                                  Goneril, gonəril.   \n",
       "3        0  In discussing the character of Hlestakov, the ...   \n",
       "4        0                               Gomalco Productions.   \n",
       "\n",
       "                                 retriever_input_ids  \\\n",
       "0  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "1  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "2  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "3  b\"e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "4  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "\n",
       "                                 generator_input_ids  \n",
       "0  b'S\\xc4\\x00\\x00n\\x01\\x00\\x00&\\x00\\x00\\x00\\xa0\\...  \n",
       "1  b'S\\xc4\\x00\\x00_\\t\\x00\\x00\\x9e\\x05\\x00\\x00R\\x0...  \n",
       "2  b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x9b8\\x00\\x00Z\\...  \n",
       "3  b'S\\xc4\\x00\\x00*\\x02\\x00\\x00\\x86+\\x00\\x00\\x06\\...  \n",
       "4  b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x80F\\x00\\x001\\...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_guttenberg = df_sentences_guttenberg.loc[\n",
    "    (df_sentences_guttenberg[\"retriever_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_guttenberg = df_sentences_guttenberg.loc[\n",
    "    (df_sentences_guttenberg[\"generator_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_guttenberg = df_sentences_guttenberg.reset_index(drop=True)\n",
    "df_sentences_guttenberg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retriever_input_ids</th>\n",
       "      <th>generator_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Sorry, Soren.\"</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x0018\\x00\\x00\\x0b\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3421</td>\n",
       "      <td>Are they alive?</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x87\\x10\\x00\\x00\\xe4\\x01\\x00\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3420</td>\n",
       "      <td>What DO you do?</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00K\\x07\\x00\\x00\\xda \\x00\\x00Y\\x01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3419</td>\n",
       "      <td>\"This is yours.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x00\\xbc\\x04\\x00\\x00&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3418</td>\n",
       "      <td>Leave the halls</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00&amp;D\\x00\\x00\\x06\\x01\\x00\\x00\\x1e_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id             text                                retriever_input_ids  \\\n",
       "0     0  \"Sorry, Soren.\"  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "1  3421  Are they alive?  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "2  3420  What DO you do?  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "3  3419  \"This is yours.  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "4  3418  Leave the halls  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "\n",
       "                                 generator_input_ids  \n",
       "0  b'S\\xc4\\x00\\x00n\\x01\\x00\\x0018\\x00\\x00\\x0b\\x00...  \n",
       "1  b'S\\xc4\\x00\\x00\\x87\\x10\\x00\\x00\\xe4\\x01\\x00\\x0...  \n",
       "2  b'S\\xc4\\x00\\x00K\\x07\\x00\\x00\\xda \\x00\\x00Y\\x01...  \n",
       "3  b'S\\xc4\\x00\\x00n\\x01\\x00\\x00\\xbc\\x04\\x00\\x00>\\...  \n",
       "4  b'S\\xc4\\x00\\x00&D\\x00\\x00\\x06\\x01\\x00\\x00\\x1e_...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_story = df_sentences_story.loc[\n",
    "    (df_sentences_story[\"retriever_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_story = df_sentences_story.loc[\n",
    "    (df_sentences_story[\"generator_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_story = df_sentences_story.reset_index(drop=True)\n",
    "df_sentences_story.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import chain\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-5\n",
    "RETRIEVER_INPUT_MAX_LENGTH = 256\n",
    "RETRIEVER_LAST_OUTPUT_SENTENCES = 3\n",
    "KNN_N_NEIGHBOURS = 4\n",
    "MAX_RELEVANT_TOKENS = 512\n",
    "LR = 1e-4\n",
    "LM_LOSS_ABG = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(retriever, retriever_tokenizer, input_ids):\n",
    "    def _prepare_ids(ids, max_length):\n",
    "        ids = list(ids)\n",
    "        if len(ids) < max_length:\n",
    "            return ids + [retriever_tokenizer.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            return ids[:max_length]\n",
    "\n",
    "    max_length = max([len(item) for item in input_ids])\n",
    "    if max_length % 8 != 0:\n",
    "        max_length += 8 - max_length % 8\n",
    "    if max_length > RETRIEVER_INPUT_MAX_LENGTH:\n",
    "        max_length = RETRIEVER_INPUT_MAX_LENGTH\n",
    "    \n",
    "    padded_input_ids = torch.LongTensor([\n",
    "        _prepare_ids(item, max_length) for item in input_ids\n",
    "    ])\n",
    "    attention_mask = padded_input_ids != retriever_tokenizer.pad_token_id\n",
    "    hidden_state = retriever(input_ids=padded_input_ids.to(retriever.device),\n",
    "                             attention_mask=attention_mask.to(retriever.device),\n",
    "                             output_hidden_states=True).last_hidden_state\n",
    "    cls_embedding = hidden_state[:, 0, :]\n",
    "    cls_embedding_norm = torch.sqrt( (cls_embedding ** 2).sum(dim=-1, keepdims=True) ) + EPS\n",
    "    return cls_embedding / cls_embedding_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(retriever, retriever_tokenizer, inputs, batch_size, verbose=False):\n",
    "    inputs = list(inputs)\n",
    "    df_sort = pd.DataFrame({\n",
    "        \"index\": range(len(inputs)),\n",
    "        \"inputs\": inputs,\n",
    "        \"length\": [len(row) for row in inputs]\n",
    "    })\n",
    "    df_sort = df_sort.sort_values(\"length\", ascending=False)\n",
    "\n",
    "    embeddings = np.zeros([len(inputs), retriever.config.hidden_size], dtype=np.float16)\n",
    "\n",
    "    batch_count = int(np.ceil(len(inputs) / batch_size))\n",
    "    with torch.no_grad():\n",
    "        iterable = range(batch_count)\n",
    "        if verbose:\n",
    "            iterable = tqdm(iterable)\n",
    "        for i in iterable:\n",
    "            batch_df_sort = df_sort.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "            batch_input_ids = batch_df_sort[\"inputs\"].apply(decode_input_ids).tolist()\n",
    "            batch_embeddings_torch = get_batch_embeddings(retriever, retriever_tokenizer, batch_input_ids)\n",
    "            embeddings[batch_df_sort[\"index\"].tolist()] = batch_embeddings_torch.detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_prompt_embeddings(retriever, generator, retriever_tokenizer, df_sentences_guttenberg, df_sentences_story):\n",
    "    print(\"UPDATING EMBEDDINGS CACHE\")\n",
    "    retriever.eval()\n",
    "    retriever.cuda()\n",
    "    extra_embeddings = get_embeddings(retriever, retriever_tokenizer, df_sentences_guttenberg[\"retriever_input_ids\"], 64, verbose=True)\n",
    "    story_embeddings = get_embeddings(retriever, retriever_tokenizer, df_sentences_story[\"retriever_input_ids\"], 64, verbose=True)\n",
    "    return extra_embeddings, story_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrieverInput:\n",
    "    input_ids: np.ndarray\n",
    "    story_sentences: np.ndarray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StoryInputSample:\n",
    "    generator_input_ids: np.ndarray\n",
    "    generator_input_weights: np.ndarray\n",
    "    retriever_inputs: List[RetrieverInput]\n",
    "\n",
    "\n",
    "def get_rows(df_story_content, id, story_id):\n",
    "    rows = []\n",
    "    df_story_content = df_story_content.loc[df_story_content[\"story_id\"] == story_id].set_index(\"id\")\n",
    "    while id != -1:\n",
    "        try:\n",
    "            row = df_story_content.loc[[id]].iloc[0]\n",
    "        except:\n",
    "            break\n",
    "        rows.append(row)\n",
    "        id = row[\"parent_id\"]\n",
    "    return rows[::-1]\n",
    "\n",
    "\n",
    "def extract_story_inputs(rows, df_sentences_context_mapping):\n",
    "    def _get_generator_inputs(row):\n",
    "        if row[\"parent_id\"] != -1:\n",
    "            row_input = list(decode_input_ids(row[\"input_generator_input_ids\"]))\n",
    "        else:\n",
    "            row_input = []\n",
    "        row_output = list(decode_input_ids(row[\"output_generator_input_ids\"]))\n",
    "        row_content = row_input + row_output\n",
    "\n",
    "        return row_content\n",
    "\n",
    "    def _get_previous_sentences(row):\n",
    "        mask = (df_sentences_context_mapping[\"story_id\"] == row[\"story_id\"]) & \\\n",
    "               (df_sentences_context_mapping[\"context_id\"] == row[\"parent_id\"])\n",
    "        return np.array(sorted(df_sentences_context_mapping.loc[mask, \"sentence_id\"]))\n",
    "\n",
    "    def _get_retriever_input_ids(rows):\n",
    "        last_row = rows[-1]\n",
    "        input_sentences_pairs = []\n",
    "        for row in rows[-2:]:\n",
    "            if row[\"parent_id\"] != -1:\n",
    "                input_ids = decode_input_ids(row[\"input_retriever_input_ids\"])\n",
    "                story_sentence_ids = _get_previous_sentences(row)\n",
    "                input_sentences_pairs.append(RetrieverInput(input_ids, story_sentence_ids))\n",
    "        return input_sentences_pairs\n",
    "\n",
    "    generator_input_ids = []\n",
    "    generator_children_counts = []\n",
    "    for row in rows:\n",
    "        row_content = _get_generator_inputs(row)\n",
    "        if row[\"children_count\"] == 0:\n",
    "            generator_children_counts.append((len(row_content), 1))\n",
    "        else:\n",
    "            generator_children_counts.append((len(row_content), row[\"children_count\"]))\n",
    "        generator_input_ids += row_content\n",
    "    generator_weights = []\n",
    "    k = 1.0\n",
    "    for i, (token_count, children_count) in enumerate(generator_children_counts[::-1]):\n",
    "        if i > 0:\n",
    "            k *= (1 / children_count)\n",
    "        generator_weights += [k] * token_count\n",
    "    generator_weights = generator_weights[::-1]\n",
    "    retriever_inputs = _get_retriever_input_ids(rows)\n",
    "    return StoryInputSample(generator_input_ids, generator_weights, retriever_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_description_encode(df_stories, story_id, generator_tokenizer):\n",
    "    tags = \"[TAGS] \" + df_stories.loc[df_stories[\"id\"] == story_id, \"tags\"].values[0]\n",
    "    return generator_tokenizer.encode(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_input(df_story_content, row_id, story_id, df_sentences_context_mapping, retriever_tokenizer):\n",
    "    rows = get_rows(df_story_content, row_id, story_id)\n",
    "    if len(rows) == 1:\n",
    "        parent_id = rows[-1][\"parent_id\"]\n",
    "    else:\n",
    "        parent_id = rows[-2][\"parent_id\"]\n",
    "    retriever_requests = [\n",
    "        (\n",
    "            decode_input_ids(rows[-1][\"input_retriever_input_ids\"]).tolist(),\n",
    "            df_sentences_context_mapping.loc[\n",
    "                (df_sentences_context_mapping[\"story_id\"] == story_id) & \\\n",
    "                (df_sentences_context_mapping[\"context_id\"] == parent_id),\n",
    "                \"sentence_id\"\n",
    "            ].tolist()\n",
    "        )\n",
    "    ]\n",
    "    if len(rows) > 1:\n",
    "        sentences_to_search = df_sentences_context_mapping.loc[\n",
    "            (df_sentences_context_mapping[\"story_id\"] == story_id) & \\\n",
    "            (df_sentences_context_mapping[\"context_id\"] == parent_id),\n",
    "            \"sentence_id\"\n",
    "        ].tolist()\n",
    "        query_sentences = nltk.sent_tokenize(rows[-2][\"output\"])[-RETRIEVER_LAST_OUTPUT_SENTENCES:]\n",
    "        retriever_requests += [\n",
    "            (retriever_tokenizer.encode(sent), sentences_to_search)\n",
    "            for sent in query_sentences\n",
    "        ]\n",
    "    return extract_story_inputs(rows, df_sentences_context_mapping), retriever_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_distance(x, y):\n",
    "    return -(x * y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_nearest_df(retriever_request_embeddings, cached_extra_nn, cached_story_nn, df_extra, df_story):\n",
    "    retriever_request_embeddings_np = retriever_request_embeddings.detach().cpu().numpy()\n",
    "    extra_indices = []\n",
    "    extra_distances = []\n",
    "    for distances, indices in zip(*cached_extra_nn.kneighbors(retriever_request_embeddings_np)):\n",
    "        extra_indices += list(indices)\n",
    "        extra_distances += list(distances)\n",
    "    sub_df_extra = df_extra.iloc[extra_indices][[\"text\", \"retriever_input_ids\", \"generator_input_ids\"]]\n",
    "    sub_df_extra[\"distance\"] = extra_distances\n",
    "\n",
    "    if cached_story_nn is not None:\n",
    "        story_indices = []\n",
    "        story_distances = []\n",
    "        for distances, indices in zip(*cached_story_nn.kneighbors(retriever_request_embeddings_np)):\n",
    "            story_indices += list(indices)\n",
    "            story_distances += list(distances)\n",
    "        sub_df_story = df_story.iloc[story_indices][[\"text\", \"retriever_input_ids\", \"generator_input_ids\"]]\n",
    "        sub_df_story[\"distance\"] = story_distances\n",
    "    \n",
    "    if cached_story_nn is not None:\n",
    "        df = pd.concat([sub_df_extra, sub_df_story]).reset_index(drop=True)\n",
    "    else:\n",
    "        df = sub_df_extra.reset_index(drop=True)\n",
    "    df = df.sort_values(\"distance\")\n",
    "    df = df.drop_duplicates(\"text\")\n",
    "    df = df.head( (RETRIEVER_LAST_OUTPUT_SENTENCES + 1) * KNN_N_NEIGHBOURS)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_generator_input(input_ids, weights, generator, generator_tokenizer, tag_input_ids, nearest_input_ids):\n",
    "    max_story_token_count = generator.config.max_position_embeddings - len(tag_input_ids) - len(nearest_input_ids)\n",
    "    generator_input_ids = input_ids[-max_story_token_count:]\n",
    "    generator_weights = weights[-max_story_token_count:]\n",
    "\n",
    "    input_tid, = generator_tokenizer.convert_tokens_to_ids([\"[INPUT]\"])\n",
    "    output_tid, = generator_tokenizer.convert_tokens_to_ids([\"[OUTPUT]\"])\n",
    "\n",
    "    if generator_input_ids[0] not in {input_tid, output_tid}:\n",
    "        if input_tid not in generator_input_ids:\n",
    "            input_start = None\n",
    "        else:\n",
    "            input_start = list(generator_input_ids).index(input_tid)\n",
    "        if output_tid not in generator_input_ids:\n",
    "            output_start = None\n",
    "        else:\n",
    "            output_start = list(generator_input_ids).index(output_tid)\n",
    "        if output_start is not None and input_start is not None:\n",
    "            if input_start < output_start:\n",
    "                start_token = output_tid\n",
    "            else:\n",
    "                start_token = input_tid\n",
    "        elif output_start is not None:\n",
    "            start_token = input_tid\n",
    "        else:\n",
    "            start_token = output_tid\n",
    "        generator_input_ids[0] = start_token\n",
    "    \n",
    "    return generator_input_ids, generator_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_input(cached_extra_nn, row, df_story_content, df_stories):\n",
    "    generator_input, retriever_requests = story_input(df_story_content,\n",
    "                                                      row[\"id\"],\n",
    "                                                      row[\"story_id\"],\n",
    "                                                      df_sentences_context_mapping,\n",
    "                                                      retriever_tokenizer)\n",
    "    story_sentence_ids = set(chain(*[sentences for _, sentences in retriever_requests]))\n",
    "    story_sentence_mask = df_sentences_story[\"id\"].isin(story_sentence_ids)\n",
    "    story_sentence_count = story_sentence_mask.sum()\n",
    "\n",
    "    if story_sentence_count > 0:\n",
    "        cached_story_nn = NearestNeighbors(n_neighbors=min(KNN_N_NEIGHBOURS, int(story_sentence_mask.sum())),\n",
    "                                        metric=dot_distance,\n",
    "                                        n_jobs=-1,\n",
    "                                        algorithm=\"brute\")\n",
    "        cached_story_nn.fit(cached_story_embeddings[story_sentence_mask])\n",
    "    else:\n",
    "        cached_story_nn = None\n",
    "\n",
    "    retriever_request_embeddings = get_batch_embeddings(retriever, retriever_tokenizer, [\n",
    "        input_ids\n",
    "        for input_ids, _ in retriever_requests\n",
    "    ])\n",
    "    df_cached_nearest = get_cached_nearest_df(retriever_request_embeddings,\n",
    "                                              cached_extra_nn,\n",
    "                                              cached_story_nn,\n",
    "                                              df_sentences_guttenberg,\n",
    "                                              df_sentences_story.loc[story_sentence_mask])\n",
    "    retriever_cached_relevant_embeddings = get_batch_embeddings(\n",
    "        retriever,\n",
    "        retriever_tokenizer,\n",
    "        df_cached_nearest[\"retriever_input_ids\"].apply(decode_input_ids)\n",
    "    )\n",
    "    retriever_distances = -retriever_request_embeddings.matmul(retriever_cached_relevant_embeddings.T)\n",
    "\n",
    "    retriever_nearest_indices = retriever_distances.mean(dim=0).sort().indices.detach().cpu().numpy()\n",
    "    df_nearest = df_cached_nearest.iloc[retriever_nearest_indices]\n",
    "    nearest_samples_input_ids = df_nearest[\"generator_input_ids\"].apply(decode_input_ids)    \n",
    "\n",
    "    nearest_input_ids = np.array(list(chain(*nearest_samples_input_ids))[:MAX_RELEVANT_TOKENS-1])\n",
    "    nearest_weights = np.zeros([len(nearest_input_ids)])\n",
    "    \n",
    "    tags_string = df_stories.loc[df_stories[\"id\"] == row[\"story_id\"], \"tags\"].values[0]\n",
    "    if pd.isna(tags_string):\n",
    "        tags_string = \"\"\n",
    "    tag_input_ids = np.array(generator_tokenizer.encode(\"[TAGS] \" + tags_string))\n",
    "    tag_weights = np.zeros([len(tag_input_ids)])\n",
    "\n",
    "    generator_input_ids, generator_weights = cut_generator_input(\n",
    "        generator_input.generator_input_ids,\n",
    "        generator_input.generator_input_weights,\n",
    "        generator,\n",
    "        generator_tokenizer,\n",
    "        tag_input_ids,\n",
    "        nearest_input_ids\n",
    "    )\n",
    "\n",
    "    input_ids = list(tag_input_ids) + list(nearest_input_ids) + list(generator_input_ids)\n",
    "    weights = list(tag_weights) + list(nearest_weights) + list(generator_weights)\n",
    "\n",
    "    return input_ids, weights, retriever_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lm_loss(input_ids, weights, logits):\n",
    "    batch_size, _, class_count = logits.shape\n",
    "    input_ids_shifted = input_ids[:, 1:].reshape([-1])\n",
    "    logits_shifted = logits[:, :-1, :].reshape([-1, class_count])\n",
    "    tokenwise_ce = F.cross_entropy(logits_shifted,\n",
    "                                   input_ids_shifted,\n",
    "                                   reduction=\"none\")\\\n",
    "        .reshape([batch_size, -1])\n",
    "    loss_samplewise = (tokenwise_ce * weights[:, 1:]).sum(dim=-1) / (weights[:, 1:].sum(dim=-1) + EPS)\n",
    "    return loss_samplewise.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_loss(cached_extra_nn, row, df_story_content_train, df_stories_train, lm_loss_avg):\n",
    "    input_ids, weights, retriever_distances = get_nn_input(cached_extra_nn,\n",
    "                                                           row,\n",
    "                                                           df_story_content_train,\n",
    "                                                           df_stories_train)\n",
    "    generator_input_ids = torch.LongTensor([input_ids]).to(generator.device)\n",
    "    generator_weights = torch.FloatTensor([weights]).to(generator.device)\n",
    "    generator_output = generator(generator_input_ids).logits\n",
    "    lm_loss = calc_lm_loss(generator_input_ids, generator_weights, generator_output)\n",
    "    distance_loss = (lm_loss - lm_loss_avg) * (1-retriever_distances).mean()\n",
    "    return lm_loss + distance_loss, lm_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationRegularizationHook():\n",
    "    def __init__(self, lambda_, norm):\n",
    "        self.regularization = 0\n",
    "        self.lambda_ = 0\n",
    "        \n",
    "    def norm(self, vec):\n",
    "        return torch.sqrt((vec ** 2).mean(dim=-1, keepdim=True) + EPS) * (torch.numel(vec) ** 2)\n",
    "        \n",
    "    def __call__(self, module, input, output):\n",
    "        try:\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                vec = output\n",
    "            elif isinstance(output, tuple) or isinstance(output, list):\n",
    "                vec = output[0]\n",
    "            self.regularization += (self.norm(vec) * self.lambda_).mean()\n",
    "        except:\n",
    "            print(f\"ERROR ON {module.__class__}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_row(regularizer_hook, optimizer, scaler, row, df_story_content_train, df_stories_train, cached_extra_nn, lm_loss_avg, lm_loss_values):\n",
    "    regularizer_hook.regularization = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        loss, lm_loss_value = get_row_loss(cached_extra_nn, row, df_story_content_train, df_stories_train, lm_loss_avg)\n",
    "        loss += regularizer_hook.regularization\n",
    "        lm_loss_values.append(lm_loss_value)\n",
    "        lm_loss_avg = sum(lm_loss_values) / len(lm_loss_values)\n",
    "        if len(lm_loss_values) == LM_LOSS_ABG:\n",
    "            lm_loss_values = lm_loss_values[-LM_LOSS_ABG:]\n",
    "    if pd.isna(loss.item()):\n",
    "        raise ValueError(\"NAN LOSS\")\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    return loss, lm_loss_value, lm_loss_values, lm_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_row(row, df_story_content_test, df_stories_test, cached_extra_nn):\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, lm_loss_value = get_row_loss(cached_extra_nn, row, df_story_content_test, df_stories_test, lm_loss_avg)\n",
    "            \n",
    "    return loss, lm_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retriever.cuda()\n",
    "generator = generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_story_content_train = df_story_content.loc[df_story_content[\"story_id\"].isin(df_stories_train[\"id\"])]\n",
    "df_story_content_train = df_story_content_train.sample(len(df_story_content_train), random_state=42)\\\n",
    "    .reset_index(drop=True)\n",
    "    \n",
    "df_story_content_test = df_story_content.loc[df_story_content[\"story_id\"].isin(df_stories_test[\"id\"])]\n",
    "df_story_content_test = df_story_content_test.sample(len(df_story_content_test), random_state=42)\\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizable_params = list(generator.parameters()) + list(retriever.parameters())\n",
    "optimizer = SGD(optimizable_params, lr=LR)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer_hook = ActivationRegularizationHook(lambda_=0.01,\n",
    "                                                norm=2)\n",
    "generator.lm_head.register_forward_hook(regularizer_hook)\n",
    "generator.transformer.wte.register_forward_hook(regularizer_hook)\n",
    "generator.transformer.wpe.register_forward_hook(regularizer_hook)\n",
    "generator.transformer.ln_f.register_forward_hook(regularizer_hook)\n",
    "for layer in generator.transformer.h:\n",
    "    layer.register_forward_hook(regularizer_hook)\n",
    "retriever.embeddings.register_forward_hook(regularizer_hook)\n",
    "for layer in retriever.encoder.layer:\n",
    "    layer.register_forward_hook(regularizer_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_CACHE_FREQ = 500\n",
    "DEBUG_SHOW_LOSS_FREQ = 25\n",
    "EPOCH_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:06<00:00, 141.37it/s]\n",
      "100%|██████████| 9749/9749 [01:07<00:00, 143.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 0 STEP 42.345367431640625 LOSS 14.614646911621094 LM LOSS\n",
      "TRAIN 25 STEP 16.06875717163086 LOSS 15.365345611572266 LM LOSS\n",
      "TRAIN 50 STEP 13.915200805664062 LOSS 14.780117263793946 LM LOSS\n",
      "TRAIN 75 STEP 11.507936172485351 LOSS 13.691031913757325 LM LOSS\n",
      "TRAIN 100 STEP 9.640334224700927 LOSS 12.760052604675293 LM LOSS\n",
      "TRAIN 125 STEP 8.96842555999756 LOSS 12.252678833007813 LM LOSS\n",
      "TRAIN 150 STEP 8.267576847076416 LOSS 11.785313148498535 LM LOSS\n",
      "TRAIN 175 STEP 8.054997215270996 LOSS 11.525365791320802 LM LOSS\n",
      "TRAIN 200 STEP 7.578642463684082 LOSS 11.193386383056641 LM LOSS\n",
      "TRAIN 225 STEP 7.5246968269348145 LOSS 11.027882385253907 LM LOSS\n",
      "TRAIN 250 STEP 6.761562652587891 LOSS 10.632028961181641 LM LOSS\n",
      "TRAIN 275 STEP 6.578518047332763 LOSS 10.441281776428223 LM LOSS\n",
      "TRAIN 300 STEP 6.508982830047607 LOSS 10.30075798034668 LM LOSS\n",
      "TRAIN 325 STEP 6.4385263633728025 LOSS 10.174543571472167 LM LOSS\n",
      "TRAIN 350 STEP 6.195426826477051 LOSS 9.99479637145996 LM LOSS\n",
      "TRAIN 375 STEP 6.051346626281738 LOSS 9.85484718322754 LM LOSS\n",
      "TRAIN 400 STEP 6.028257598876953 LOSS 9.765047607421875 LM LOSS\n",
      "TRAIN 425 STEP 5.930599975585937 LOSS 9.654091529846191 LM LOSS\n",
      "TRAIN 450 STEP 6.2670560073852535 LOSS 9.697114295959473 LM LOSS\n",
      "TRAIN 475 STEP 5.876782779693603 LOSS 9.499944305419922 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:07<00:00, 140.84it/s]\n",
      "100%|██████████| 9749/9749 [01:08<00:00, 142.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 500 STEP 6.291034011840821 LOSS 9.57885944366455 LM LOSS\n",
      "TRAIN 525 STEP 6.274194030761719 LOSS 9.519052963256836 LM LOSS\n",
      "TRAIN 550 STEP 5.922775917053222 LOSS 9.34669994354248 LM LOSS\n",
      "TRAIN 575 STEP 5.751086921691894 LOSS 9.237149047851563 LM LOSS\n",
      "TRAIN 600 STEP 5.324308567047119 LOSS 9.04067741394043 LM LOSS\n",
      "TRAIN 625 STEP 5.761481189727784 LOSS 9.138587036132812 LM LOSS\n",
      "TRAIN 650 STEP 5.462719058990478 LOSS 8.99299861907959 LM LOSS\n",
      "TRAIN 675 STEP 5.722183761596679 LOSS 9.035183658599854 LM LOSS\n",
      "TRAIN 700 STEP 5.3896791458129885 LOSS 8.881279277801514 LM LOSS\n",
      "TRAIN 725 STEP 5.461398258209228 LOSS 8.863807106018067 LM LOSS\n",
      "TRAIN 750 STEP 5.7200017929077145 LOSS 8.91253074645996 LM LOSS\n",
      "TRAIN 775 STEP 5.883830299377442 LOSS 8.931662940979004 LM LOSS\n",
      "TRAIN 800 STEP 6.097068729400635 LOSS 8.97315860748291 LM LOSS\n",
      "TRAIN 825 STEP 5.500218677520752 LOSS 8.740656070709228 LM LOSS\n",
      "TRAIN 850 STEP 5.954860324859619 LOSS 8.861541633605958 LM LOSS\n",
      "TRAIN 875 STEP 5.984619178771973 LOSS 8.84393527984619 LM LOSS\n",
      "TRAIN 900 STEP 5.474086685180664 LOSS 8.643925666809082 LM LOSS\n",
      "TRAIN 925 STEP 5.776343460083008 LOSS 8.716644744873047 LM LOSS\n",
      "TRAIN 950 STEP 5.478861865997314 LOSS 8.588196334838868 LM LOSS\n",
      "TRAIN 975 STEP 5.666790199279785 LOSS 8.62514144897461 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:07<00:00, 140.71it/s]\n",
      "100%|██████████| 9749/9749 [01:07<00:00, 143.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 1000 STEP 5.285780715942383 LOSS 8.470765705108642 LM LOSS\n",
      "TRAIN 1025 STEP 6.2773100280761716 LOSS 8.779853324890137 LM LOSS\n",
      "TRAIN 1050 STEP 5.82645616531372 LOSS 8.608991298675537 LM LOSS\n",
      "TRAIN 1075 STEP 5.645462474822998 LOSS 8.524920749664307 LM LOSS\n",
      "TRAIN 1100 STEP 6.0704961776733395 LOSS 8.645378112792969 LM LOSS\n",
      "TRAIN 1125 STEP 5.490908527374268 LOSS 8.430514793395997 LM LOSS\n",
      "TRAIN 1150 STEP 5.883071002960205 LOSS 8.538321266174316 LM LOSS\n",
      "TRAIN 1175 STEP 5.200051879882812 LOSS 8.291884899139404 LM LOSS\n",
      "TRAIN 1200 STEP 5.93543417930603 LOSS 8.515308647155761 LM LOSS\n",
      "TRAIN 1225 STEP 5.342792148590088 LOSS 8.299985198974609 LM LOSS\n",
      "TRAIN 1250 STEP 5.839415893554688 LOSS 8.443016452789307 LM LOSS\n",
      "TRAIN 1275 STEP 5.722534065246582 LOSS 8.38952621459961 LM LOSS\n",
      "TRAIN 1300 STEP 5.961051330566407 LOSS 8.448735485076904 LM LOSS\n",
      "TRAIN 1325 STEP 5.400311203002929 LOSS 8.2477095413208 LM LOSS\n",
      "TRAIN 1350 STEP 5.8868835830688475 LOSS 8.3947172164917 LM LOSS\n",
      "TRAIN 1375 STEP 5.377301959991455 LOSS 8.206628398895264 LM LOSS\n",
      "TRAIN 1400 STEP 5.626850299835205 LOSS 8.27414737701416 LM LOSS\n",
      "TRAIN 1425 STEP 5.943898992538452 LOSS 8.365501441955566 LM LOSS\n",
      "TRAIN 1450 STEP 5.969287757873535 LOSS 8.360506763458252 LM LOSS\n",
      "TRAIN 1475 STEP 5.807489204406738 LOSS 8.290136871337891 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:07<00:00, 140.71it/s]\n",
      "100%|██████████| 9749/9749 [01:08<00:00, 142.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 1500 STEP 5.577840671539307 LOSS 8.199720764160157 LM LOSS\n",
      "TRAIN 1525 STEP 6.2095866203308105 LOSS 8.399104442596435 LM LOSS\n",
      "TRAIN 1550 STEP 5.35922986984253 LOSS 8.099901561737061 LM LOSS\n",
      "TRAIN 1575 STEP 6.0139931869506835 LOSS 8.305445671081543 LM LOSS\n",
      "TRAIN 1600 STEP 5.375048370361328 LOSS 8.077942905426026 LM LOSS\n",
      "TRAIN 1625 STEP 5.428056964874267 LOSS 8.08217197418213 LM LOSS\n",
      "TRAIN 1650 STEP 5.638217487335205 LOSS 8.140021381378174 LM LOSS\n",
      "TRAIN 1675 STEP 5.6265151405334475 LOSS 8.122468643188476 LM LOSS\n",
      "TRAIN 1700 STEP 5.654837245941162 LOSS 8.120761814117431 LM LOSS\n",
      "TRAIN 1725 STEP 6.069045085906982 LOSS 8.248547630310059 LM LOSS\n",
      "TRAIN 1750 STEP 5.297185153961181 LOSS 7.976441478729248 LM LOSS\n",
      "TRAIN 1775 STEP 5.7987015914916995 LOSS 8.13309404373169 LM LOSS\n",
      "TRAIN 1800 STEP 5.469475288391113 LOSS 8.012092876434327 LM LOSS\n",
      "TRAIN 1825 STEP 5.920760183334351 LOSS 8.152511386871337 LM LOSS\n",
      "TRAIN 1850 STEP 5.872453708648681 LOSS 8.125298080444336 LM LOSS\n",
      "TRAIN 1875 STEP 5.702555065155029 LOSS 8.05845401763916 LM LOSS\n",
      "TRAIN 1900 STEP 5.819716796875 LOSS 8.086854267120362 LM LOSS\n",
      "TRAIN 1925 STEP 6.057471427917481 LOSS 8.157420902252197 LM LOSS\n",
      "TRAIN 1950 STEP 5.528082618713379 LOSS 7.96983226776123 LM LOSS\n",
      "TRAIN 1975 STEP 5.991996326446533 LOSS 8.11561248779297 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:07<00:00, 140.82it/s]\n",
      "100%|██████████| 9749/9749 [01:07<00:00, 143.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 2000 STEP 6.0471262550354 LOSS 8.124854049682618 LM LOSS\n",
      "TRAIN 2025 STEP 5.602674245834351 LOSS 7.967186489105225 LM LOSS\n",
      "TRAIN 2050 STEP 5.713523178100586 LOSS 7.994241065979004 LM LOSS\n",
      "TRAIN 2075 STEP 5.913788652420044 LOSS 8.05210865020752 LM LOSS\n",
      "TRAIN 2100 STEP 5.676249561309814 LOSS 7.963091068267822 LM LOSS\n",
      "TRAIN 2125 STEP 5.5727263450622555 LOSS 7.919491329193115 LM LOSS\n",
      "TRAIN 2150 STEP 5.911755199432373 LOSS 8.025139331817627 LM LOSS\n",
      "TRAIN 2175 STEP 5.794169063568115 LOSS 7.977925891876221 LM LOSS\n",
      "TRAIN 2200 STEP 5.904608306884765 LOSS 8.005891647338867 LM LOSS\n",
      "TRAIN 2225 STEP 5.88867073059082 LOSS 7.99313325881958 LM LOSS\n",
      "TRAIN 2250 STEP 5.454353914260865 LOSS 7.838727798461914 LM LOSS\n",
      "TRAIN 2275 STEP 5.529524459838867 LOSS 7.855011463165283 LM LOSS\n",
      "TRAIN 2300 STEP 6.128035011291504 LOSS 8.047733554840088 LM LOSS\n",
      "TRAIN 2325 STEP 6.261982870101929 LOSS 8.085675849914551 LM LOSS\n",
      "TRAIN 2350 STEP 5.288256816864013 LOSS 7.752839984893799 LM LOSS\n",
      "TRAIN 2375 STEP 6.008084297180176 LOSS 7.982668228149414 LM LOSS\n",
      "TRAIN 2400 STEP 6.165367698669433 LOSS 8.030497951507568 LM LOSS\n",
      "TRAIN 2425 STEP 6.626146183013916 LOSS 8.179476680755615 LM LOSS\n",
      "TRAIN 2450 STEP 5.7076406669616695 LOSS 7.865037002563477 LM LOSS\n",
      "TRAIN 2475 STEP 6.240712184906005 LOSS 8.037910823822022 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:07<00:00, 140.63it/s]\n",
      "100%|██████████| 9749/9749 [01:07<00:00, 143.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 2500 STEP 5.605777702331543 LOSS 7.818160629272461 LM LOSS\n",
      "TRAIN 2525 STEP 5.802825775146484 LOSS 7.875544300079346 LM LOSS\n",
      "TRAIN 2550 STEP 5.81982988357544 LOSS 7.8761794090271 LM LOSS\n",
      "TRAIN 2575 STEP 5.787337322235107 LOSS 7.856251449584961 LM LOSS\n",
      "TRAIN 2600 STEP 5.666589126586914 LOSS 7.810858955383301 LM LOSS\n",
      "TRAIN 2625 STEP 6.33527774810791 LOSS 8.027971019744873 LM LOSS\n",
      "TRAIN 2650 STEP 5.992916698455811 LOSS 7.908023109436035 LM LOSS\n",
      "TRAIN 2675 STEP 5.901295719146728 LOSS 7.869446640014648 LM LOSS\n",
      "TRAIN 2700 STEP 5.824738416671753 LOSS 7.839159774780273 LM LOSS\n",
      "TRAIN 2725 STEP 5.627282447814942 LOSS 7.766449642181397 LM LOSS\n",
      "TRAIN 2750 STEP 6.448565063476562 LOSS 8.031916980743409 LM LOSS\n",
      "TRAIN 2775 STEP 5.616409482955933 LOSS 7.751657581329345 LM LOSS\n",
      "TRAIN 2800 STEP 5.843920402526855 LOSS 7.8221147346496585 LM LOSS\n",
      "TRAIN 2825 STEP 5.624302625656128 LOSS 7.741888999938965 LM LOSS\n",
      "TRAIN 2850 STEP 5.800096950531006 LOSS 7.795652198791504 LM LOSS\n",
      "TRAIN 2875 STEP 5.970415153503418 LOSS 7.8454473686218265 LM LOSS\n",
      "TRAIN 2900 STEP 6.2019454383850094 LOSS 7.919325466156006 LM LOSS\n",
      "TRAIN 2925 STEP 5.658590106964112 LOSS 7.730595359802246 LM LOSS\n",
      "TRAIN 2950 STEP 6.0778529167175295 LOSS 7.865348663330078 LM LOSS\n",
      "TRAIN 2975 STEP 6.539254932403565 LOSS 8.015331497192383 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:07<00:00, 140.54it/s]\n",
      "100%|██████████| 9749/9749 [01:08<00:00, 142.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 3000 STEP 5.836252498626709 LOSS 7.774411735534668 LM LOSS\n",
      "TRAIN 3025 STEP 5.815690822601319 LOSS 7.7636525917053225 LM LOSS\n",
      "TRAIN 3050 STEP 6.808391056060791 LOSS 8.091607704162598 LM LOSS\n",
      "TRAIN 3075 STEP 6.022694244384765 LOSS 7.8242738151550295 LM LOSS\n",
      "TRAIN 3100 STEP 6.0505428123474125 LOSS 7.829192428588867 LM LOSS\n",
      "TRAIN 3125 STEP 6.17789363861084 LOSS 7.866553001403808 LM LOSS\n",
      "TRAIN 3150 STEP 5.470148086547852 LOSS 7.6246271896362305 LM LOSS\n",
      "TRAIN 3175 STEP 5.677520427703858 LOSS 7.6876483726501466 LM LOSS\n",
      "TRAIN 3200 STEP 6.741142311096191 LOSS 8.040759315490723 LM LOSS\n",
      "TRAIN 3225 STEP 6.190406360626221 LOSS 7.852140655517578 LM LOSS\n",
      "TRAIN 3250 STEP 5.695697450637818 LOSS 7.681447696685791 LM LOSS\n",
      "TRAIN 3275 STEP 6.165540733337402 LOSS 7.834061203002929 LM LOSS\n",
      "TRAIN 3300 STEP 6.335865888595581 LOSS 7.887102870941162 LM LOSS\n",
      "TRAIN 3325 STEP 6.2766912651062015 LOSS 7.8636489105224605 LM LOSS\n",
      "TRAIN 3350 STEP 6.2557646179199216 LOSS 7.852438144683838 LM LOSS\n",
      "TRAIN 3375 STEP 5.617485189437867 LOSS 7.634233589172363 LM LOSS\n",
      "TRAIN 3400 STEP 6.103247261047363 LOSS 7.790699348449707 LM LOSS\n",
      "TRAIN 3425 STEP 6.348084268569946 LOSS 7.870227489471436 LM LOSS\n",
      "TRAIN 3450 STEP 5.754615478515625 LOSS 7.6677766609191895 LM LOSS\n",
      "TRAIN 3475 STEP 5.981796941757202 LOSS 7.739395866394043 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:07<00:00, 140.57it/s]\n",
      "100%|██████████| 9749/9749 [01:08<00:00, 143.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 3500 STEP 5.872775726318359 LOSS 7.697750511169434 LM LOSS\n",
      "TRAIN 3525 STEP 6.1769644641876225 LOSS 7.795930080413818 LM LOSS\n",
      "TRAIN 3550 STEP 6.075185642242432 LOSS 7.757721271514892 LM LOSS\n",
      "TRAIN 3575 STEP 6.141312885284424 LOSS 7.776526069641113 LM LOSS\n",
      "TRAIN 3600 STEP 5.514448394775391 LOSS 7.562037925720215 LM LOSS\n",
      "TRAIN 3625 STEP 5.503444814682007 LOSS 7.553440208435059 LM LOSS\n",
      "TRAIN 3650 STEP 6.308185176849365 LOSS 7.818919410705567 LM LOSS\n",
      "TRAIN 3675 STEP 5.7828850746154785 LOSS 7.639037189483642 LM LOSS\n",
      "TRAIN 3700 STEP 5.943308353424072 LOSS 7.6888906860351565 LM LOSS\n",
      "TRAIN 3725 STEP 6.348103675842285 LOSS 7.820725154876709 LM LOSS\n",
      "TRAIN 3750 STEP 5.6312578010559085 LOSS 7.576742343902588 LM LOSS\n",
      "TRAIN 3775 STEP 6.057567405700683 LOSS 7.715998744964599 LM LOSS\n",
      "TRAIN 3800 STEP 5.855609931945801 LOSS 7.6438471221923825 LM LOSS\n",
      "TRAIN 3825 STEP 6.072942028045654 LOSS 7.712748985290528 LM LOSS\n",
      "TRAIN 3850 STEP 6.088807258605957 LOSS 7.7146938514709475 LM LOSS\n",
      "TRAIN 3875 STEP 5.922831754684449 LOSS 7.65571361541748 LM LOSS\n",
      "TRAIN 3900 STEP 6.021020736694336 LOSS 7.684921627044678 LM LOSS\n",
      "TRAIN 3925 STEP 5.746358499526978 LOSS 7.589280242919922 LM LOSS\n",
      "TRAIN 3950 STEP 6.494477958679199 LOSS 7.836806468963623 LM LOSS\n",
      "TRAIN 3975 STEP 6.123941230773926 LOSS 7.708593387603759 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:08<00:00, 139.85it/s]\n",
      "100%|██████████| 9749/9749 [01:08<00:00, 141.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 4000 STEP 6.230025386810302 LOSS 7.741507434844971 LM LOSS\n",
      "TRAIN 4025 STEP 5.932142677307129 LOSS 7.638251075744629 LM LOSS\n",
      "TRAIN 4050 STEP 5.815835037231445 LOSS 7.595049324035645 LM LOSS\n",
      "TRAIN 4075 STEP 5.667302093505859 LOSS 7.541551284790039 LM LOSS\n",
      "TRAIN 4100 STEP 5.748593406677246 LOSS 7.565633811950684 LM LOSS\n",
      "TRAIN 4125 STEP 6.308592090606689 LOSS 7.750182762145996 LM LOSS\n",
      "TRAIN 4150 STEP 6.219777297973633 LOSS 7.717616062164307 LM LOSS\n",
      "TRAIN 4175 STEP 6.1302375221252445 LOSS 7.6844588661193844 LM LOSS\n",
      "TRAIN 4200 STEP 5.947985630035401 LOSS 7.619593544006348 LM LOSS\n",
      "TRAIN 4225 STEP 6.043252964019775 LOSS 7.648396167755127 LM LOSS\n",
      "TRAIN 4250 STEP 6.101083965301513 LOSS 7.664985809326172 LM LOSS\n",
      "TRAIN 4275 STEP 6.0404543113708495 LOSS 7.641507110595703 LM LOSS\n",
      "TRAIN 4300 STEP 6.029890098571777 LOSS 7.634766654968262 LM LOSS\n",
      "TRAIN 4325 STEP 5.730754814147949 LOSS 7.530834197998047 LM LOSS\n",
      "TRAIN 4350 STEP 5.825833368301391 LOSS 7.562921142578125 LM LOSS\n",
      "TRAIN 4375 STEP 5.717927417755127 LOSS 7.519429664611817 LM LOSS\n",
      "TRAIN 4400 STEP 6.0083888816833495 LOSS 7.613941783905029 LM LOSS\n",
      "TRAIN 4425 STEP 5.824152069091797 LOSS 7.548896217346192 LM LOSS\n",
      "TRAIN 4450 STEP 5.683192539215088 LOSS 7.499152545928955 LM LOSS\n",
      "TRAIN 4475 STEP 6.100186367034912 LOSS 7.635215072631836 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:07<00:00, 140.90it/s]\n",
      "100%|██████████| 9749/9749 [01:08<00:00, 142.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 4500 STEP 5.851972084045411 LOSS 7.549555015563965 LM LOSS\n",
      "TRAIN 4525 STEP 6.159712429046631 LOSS 7.6484160995483395 LM LOSS\n",
      "TRAIN 4550 STEP 5.973983030319214 LOSS 7.584162464141846 LM LOSS\n",
      "TRAIN 4575 STEP 6.123812007904053 LOSS 7.631247577667236 LM LOSS\n",
      "TRAIN 4600 STEP 5.7300255012512205 LOSS 7.496329689025879 LM LOSS\n",
      "TRAIN 4625 STEP 5.803626546859741 LOSS 7.518239498138428 LM LOSS\n",
      "TRAIN 4650 STEP 6.198034248352051 LOSS 7.647565383911132 LM LOSS\n",
      "TRAIN 4675 STEP 6.298184604644775 LOSS 7.680878524780273 LM LOSS\n",
      "TRAIN 4700 STEP 6.1827425861358645 LOSS 7.637058925628662 LM LOSS\n",
      "TRAIN 4725 STEP 5.959965782165527 LOSS 7.5598844718933105 LM LOSS\n",
      "TRAIN 4750 STEP 6.354040336608887 LOSS 7.689124603271484 LM LOSS\n",
      "TRAIN 4775 STEP 5.5892191505432125 LOSS 7.4336646461486815 LM LOSS\n",
      "TRAIN 4800 STEP 5.869840316772461 LOSS 7.521430320739746 LM LOSS\n",
      "TRAIN 4825 STEP 6.369626770019531 LOSS 7.6858173370361325 LM LOSS\n",
      "TRAIN 4850 STEP 5.824019155502319 LOSS 7.500734786987305 LM LOSS\n",
      "TRAIN 4875 STEP 6.415931339263916 LOSS 7.696232585906983 LM LOSS\n",
      "TRAIN 4900 STEP 6.553478517532349 LOSS 7.739772033691406 LM LOSS\n",
      "TRAIN 4925 STEP 6.153601131439209 LOSS 7.603281860351562 LM LOSS\n",
      "TRAIN 4950 STEP 6.045076122283936 LOSS 7.565400848388672 LM LOSS\n",
      "TRAIN 4975 STEP 6.339558925628662 LOSS 7.661651058197021 LM LOSS\n",
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:06<00:00, 141.09it/s]\n",
      "100%|██████████| 9749/9749 [01:08<00:00, 142.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 5000 STEP 6.644211254119873 LOSS 7.761436042785644 LM LOSS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 7136/8772 [1:15:27<17:28,  1.56it/s]"
     ]
    }
   ],
   "source": [
    "i = -1\n",
    "lm_loss_values = []\n",
    "lm_loss_avg = 0.0\n",
    "j = -1\n",
    "best_loss = np.inf\n",
    "loss_values_mean_container = []\n",
    "lm_loss_values_mean_container = []\n",
    "while True:\n",
    "    i += 1\n",
    "    j += 1\n",
    "    if i == len(df_story_content_train):\n",
    "        i = 0\n",
    "    if i % UPDATE_CACHE_FREQ == 0:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            cached_extra_embeddings, cached_story_embeddings = get_cached_prompt_embeddings(retriever,\n",
    "                                                                                            generator,\n",
    "                                                                                            retriever_tokenizer,\n",
    "                                                                                            df_sentences_guttenberg,\n",
    "                                                                                            df_sentences_story)\n",
    "            cached_extra_nn = NearestNeighbors(n_neighbors=KNN_N_NEIGHBOURS, metric=dot_distance, n_jobs=-1, algorithm=\"ball_tree\")\n",
    "            cached_extra_nn.fit(cached_extra_embeddings)\n",
    "            retriever.train()\n",
    "            generator.train()\n",
    "    row = df_story_content_train.iloc[i]\n",
    "    loss, lm_loss_value, lm_loss_values, lm_loss_avg = train_row(regularizer_hook,\n",
    "                                                                 optimizer,\n",
    "                                                                 scaler,\n",
    "                                                                 row,\n",
    "                                                                 df_story_content_train,\n",
    "                                                                 df_stories_train,\n",
    "                                                                 cached_extra_nn,\n",
    "                                                                 lm_loss_avg,\n",
    "                                                                 lm_loss_values)\n",
    "    loss_values_mean_container.append(loss.item())\n",
    "    lm_loss_values_mean_container.append(lm_loss_value)\n",
    "    if i % DEBUG_SHOW_LOSS_FREQ == 0:\n",
    "        print(f\"TRAIN {j} STEP {sum(loss_values_mean_container) / len(loss_values_mean_container)} LOSS {sum(lm_loss_values_mean_container) / len(lm_loss_values_mean_container)} LM LOSS\")\n",
    "        loss_values_mean_container = []\n",
    "        lm_loss_values_mean_container = []\n",
    "        \n",
    "    if j > 0 and j % EPOCH_SIZE == 0:\n",
    "        epoch = j // EPOCH_SIZE\n",
    "        val_loss_values = []\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                for _, row in tqdm(df_story_content_test.iterrows(), total=len(df_story_content_test)):\n",
    "                    _, lm_loss_value = validation_row(row,\n",
    "                                                 df_story_content_test,\n",
    "                                                 df_stories_test,\n",
    "                                                 cached_extra_nn)\n",
    "                    val_loss_values.append(lm_loss_value)\n",
    "        val_loss = np.array(val_loss_values).mean()\n",
    "        print(f\"VALIDATION EPOCH {epoch} LOSS {val_loss}\")\n",
    "        with open(\"train-log-base.txt\", \"a\") as log_target:\n",
    "            log_target.write(f\"VALIDATION EPOCH {epoch} LOSS {val_loss}\\n\")\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            retriever.save_pretrained(\"checkpoint-retriever-base\")\n",
    "            generator.save_pretrained(\"checkpoint-generator-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('checkpoint-retriever-base/tokenizer_config.json',\n",
       " 'checkpoint-retriever-base/special_tokens_map.json',\n",
       " 'checkpoint-retriever-base/vocab.txt',\n",
       " 'checkpoint-retriever-base/added_tokens.json',\n",
       " 'checkpoint-retriever-base/tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tokenizer.save_pretrained(\"checkpoint-retriever-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('checkpoint-generator-base/tokenizer_config.json',\n",
       " 'checkpoint-generator-base/special_tokens_map.json',\n",
       " 'checkpoint-generator-base/vocab.json',\n",
       " 'checkpoint-generator-base/merges.txt',\n",
       " 'checkpoint-generator-base/added_tokens.json',\n",
       " 'checkpoint-generator-base/tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_tokenizer.save_pretrained(\"checkpoint-generator-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  adding: checkpoint-generator-base/ (stored 0%)\n",
      "  adding: checkpoint-generator-base/config.json (deflated 62%)\n",
      "  adding: checkpoint-generator-base/special_tokens_map.json (deflated 52%)\n",
      "  adding: checkpoint-generator-base/merges.txt (deflated 53%)\n",
      "  adding: checkpoint-generator-base/tokenizer_config.json (deflated 63%)\n",
      "  adding: checkpoint-generator-base/added_tokens.json (deflated 41%)\n",
      "  adding: checkpoint-generator-base/tokenizer.json (deflated 59%)\n",
      "  adding: checkpoint-generator-base/pytorch_model.bin (deflated 11%)\n",
      "  adding: checkpoint-generator-base/vocab.json (deflated 59%)\n",
      "  adding: checkpoint-retriever-base/ (stored 0%)\n",
      "  adding: checkpoint-retriever-base/config.json (deflated 47%)\n",
      "  adding: checkpoint-retriever-base/special_tokens_map.json (deflated 40%)\n",
      "  adding: checkpoint-retriever-base/vocab.txt (deflated 53%)\n",
      "  adding: checkpoint-retriever-base/tokenizer_config.json (deflated 41%)\n",
      "  adding: checkpoint-retriever-base/added_tokens.json (deflated 41%)\n",
      "  adding: checkpoint-retriever-base/tokenizer.json (deflated 59%)\n",
      "  adding: checkpoint-retriever-base/pytorch_model.bin (deflated 7%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r \"base-models.zip\" \"checkpoint-generator-base/\" \"checkpoint-retriever-base/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httplib2==0.15.0\n",
      "  Downloading httplib2-0.15.0-py3-none-any.whl (94 kB)\n",
      "     |████████████████████████████████| 94 kB 3.0 MB/s             \n",
      "\u001b[?25hInstalling collected packages: httplib2\n",
      "  Attempting uninstall: httplib2\n",
      "    Found existing installation: httplib2 0.20.1\n",
      "    Uninstalling httplib2-0.20.1:\n",
      "      Successfully uninstalled httplib2-0.20.1\n",
      "Successfully installed httplib2-0.15.0\n",
      "Collecting google-api-python-client==1.6\n",
      "  Downloading google_api_python_client-1.6.0-py2.py3-none-any.whl (52 kB)\n",
      "     |████████████████████████████████| 52 kB 1.3 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.6) (1.16.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.6) (0.15.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.6) (3.0.1)\n",
      "Requirement already satisfied: oauth2client<5.0.0dev,>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.6) (4.1.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.2.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (4.7.2)\n",
      "Installing collected packages: google-api-python-client\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.27.0\n",
      "    Uninstalling google-api-python-client-2.27.0:\n",
      "      Successfully uninstalled google-api-python-client-2.27.0\n",
      "Successfully installed google-api-python-client-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install httplib2==0.15.0\n",
    "!pip install google-api-python-client==1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcloud import storage\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client(project='realm-dungeon')\n",
    "bucket = client.get_bucket('realm-dungeon-models')\n",
    "blob = bucket.blob('base-models.zip')\n",
    "blob.upload_from_filename('base-models.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "interpreter": {
   "hash": "14491e825d4673210f59c83732ae5f4b564c5c040b5b1d881a577d4827971b6d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
