{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.12.5 in /opt/conda/lib/python3.7/site-packages (4.12.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2021.10.21)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.1.2)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (3.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.8.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (1.19.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.5) (3.6.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "Requirement already satisfied: pandas==1.3.4 in /opt/conda/lib/python3.7/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.4) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
      "Requirement already satisfied: tqdm==4.62.3 in /opt/conda/lib/python3.7/site-packages (4.62.3)\n",
      "Requirement already satisfied: nltk==3.6.5 in /opt/conda/lib/python3.7/site-packages (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (2021.10.21)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk==3.6.5) (8.0.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk==3.6.5) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.5) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk==3.6.5) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.12.5\n",
    "!pip install pandas==1.3.4\n",
    "!pip install tqdm==4.62.3\n",
    "!pip install nltk==3.6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, GPT2TokenizerFast, BertModel, GPTNeoForCausalLM\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVER_BERT_MODEL = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "GENERATOR_GPTNEO_MODEL = \"EleutherAI/gpt-neo-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tokenizer = BertTokenizerFast.from_pretrained(RETRIEVER_BERT_MODEL)\n",
    "retriever_tokenizer.add_tokens([\"[STORY]\", \"[EXTRA]\", \"[RETRIEVE]\"])\n",
    "\n",
    "generator_tokenizer = GPT2TokenizerFast.from_pretrained(GENERATOR_GPTNEO_MODEL)\n",
    "generator_tokenizer.add_tokens([\"[TAGS]\", \"[INIT]\", \"[PROMPT]\", \"[TEXT]\", \"[INPUT]\", \"[OUTPUT]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['fit_denses.0.bias', 'fit_denses.2.weight', 'fit_denses.1.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.1.weight', 'fit_denses.4.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.0.weight', 'fit_denses.2.bias', 'fit_denses.4.weight', 'cls.predictions.decoder.weight', 'fit_denses.3.bias', 'fit_denses.3.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "retriever = BertModel.from_pretrained(RETRIEVER_BERT_MODEL)\n",
    "generator = GPTNeoForCausalLM.from_pretrained(GENERATOR_GPTNEO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_tokens(tokenizer, model):\n",
    "    if len(tokenizer) % 8 != 0:\n",
    "        tokenizer.add_tokens([\n",
    "            f\"[DUMB{i}]\"\n",
    "            for i in range(8 - len(tokenizer) % 8)\n",
    "        ])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tokenizer, retriever = sync_tokens(retriever_tokenizer, retriever)\n",
    "generator_tokenizer, generator = sync_tokens(generator_tokenizer, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_apply(df, func, chunk_size, process_count):\n",
    "    chunk_count = int(np.ceil(len(df) / chunk_size))\n",
    "    return list(chain(*Parallel(n_jobs=process_count)(\n",
    "        delayed(func)(df.iloc[i * chunk_size : (i + 1) * chunk_size])\n",
    "        for i in tqdm(range(chunk_count))\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_ids(input_ids):\n",
    "    return np.array(input_ids, dtype=np.int32).tobytes()\n",
    "\n",
    "def decode_input_ids(buffer):\n",
    "    return np.frombuffer(buffer, dtype=np.int32)\n",
    "\n",
    "def apply_tokenizer(texts, tokenizer):\n",
    "    return [\n",
    "        encode_input_ids(row)\n",
    "        for row in tokenizer(list(texts))[\"input_ids\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences_guttenberg = pd.read_csv(\"data/guttenberg-sentences-sampled.csv\")\n",
    "df_sentences_story = pd.read_csv(\"data/cleaned/story-sentences.csv\")\n",
    "df_sentences_context_mapping = pd.read_csv(\"data/cleaned/story-context-sentence-mapping-numeric-id.csv\")\n",
    "df_story_content = pd.read_csv(\"data/cleaned/story-trees-numeric-id.csv\")\n",
    "df_stories_train = pd.read_csv(\"data/cleaned/stories-train.csv\")\n",
    "df_stories_test = pd.read_csv(\"data/cleaned/stories-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>story_id</th>\n",
       "      <th>children_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>The land of Kronnland is a mythical, wonderful...</td>\n",
       "      <td>12487</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Start Danny's Campaign</td>\n",
       "      <td>Danny Blaze\\nBackground :\\nBorn in the summer ...</td>\n",
       "      <td>12487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Continue</td>\n",
       "      <td>With all the townsfolk transformed into mindle...</td>\n",
       "      <td>12487</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Get back to Bren and warn him about the danger.</td>\n",
       "      <td>You run down the hill as Andrew's army regroup...</td>\n",
       "      <td>12487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Watch the battle from your hideout.</td>\n",
       "      <td>Although worried, you stay in your hideout and...</td>\n",
       "      <td>12487</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  parent_id                                            input  \\\n",
       "0   0         -1                                           [ROOT]   \n",
       "1   1          0                          Start Danny's Campaign    \n",
       "2   2          1                                         Continue   \n",
       "3   3          2  Get back to Bren and warn him about the danger.   \n",
       "4   4          2              Watch the battle from your hideout.   \n",
       "\n",
       "                                              output  story_id  children_count  \n",
       "0  The land of Kronnland is a mythical, wonderful...     12487               3  \n",
       "1  Danny Blaze\\nBackground :\\nBorn in the summer ...     12487               1  \n",
       "2  With all the townsfolk transformed into mindle...     12487               2  \n",
       "3  You run down the hill as Andrew's army regroup...     12487               0  \n",
       "4  Although worried, you stay in your hideout and...     12487               2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2children_count = df_story_content.groupby([\"story_id\", \"parent_id\"])[\"id\"].nunique().to_dict()\n",
    "df_story_content[\"children_count\"] = df_story_content[[\"story_id\", \"id\"]].apply(\n",
    "    lambda row: id2children_count.get((row[\"story_id\"], row[\"id\"]), 0),\n",
    "    axis=1\n",
    ")\n",
    "df_story_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:28<00:00,  6.42it/s]\n",
      " 65%|██████▌   | 120/184 [00:19<00:11,  5.53it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2103 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 184/184 [00:30<00:00,  5.97it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sentences_guttenberg[\"retriever_input_ids\"] = parallel_apply(\n",
    "    df_sentences_guttenberg,\n",
    "    lambda df: apply_tokenizer(\"[EXTRA] \" + df[\"text\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_sentences_guttenberg[\"generator_input_ids\"] = parallel_apply(\n",
    "    df_sentences_guttenberg,\n",
    "    lambda df: apply_tokenizer(\"[PROMPT] \" + df[\"text\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:04<00:00, 15.51it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.84it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sentences_story[\"retriever_input_ids\"] = parallel_apply(\n",
    "    df_sentences_story,\n",
    "    lambda df: apply_tokenizer(\"[STORY] \" + df[\"text\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_sentences_story[\"generator_input_ids\"] = parallel_apply(\n",
    "    df_sentences_story,\n",
    "    lambda df: apply_tokenizer(\"[PROMPT] \" + df[\"text\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 6048.65it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 5728.81it/s]\n"
     ]
    }
   ],
   "source": [
    "df_story_content[\"input_retriever_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[RETRIEVE] \" + df[\"input\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_story_content[\"input_generator_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[INPUT] \" + df[\"input\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 6087.52it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2157 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2219 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2362 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2056 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2105 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2088 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3423 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 7/7 [00:00<00:00, 5872.03it/s]\n"
     ]
    }
   ],
   "source": [
    "df_story_content[\"output_generator_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[OUTPUT] \" + df[\"output\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_story_content[\"output_retriever_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[RETRIEVE] \" + df[\"output\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>text</th>\n",
       "      <th>retriever_input_ids</th>\n",
       "      <th>generator_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Gobryas is there?\"</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x00&amp;\\x00\\x00\\x00\\xa0\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>His name's Gonzago.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00_\\t\\x00\\x00\\x9e\\x05\\x00\\x00R\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Goneril, gonəril.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x9b8\\x00\\x00Z\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>In discussing the character of Hlestakov, the ...</td>\n",
       "      <td>b\"e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00*\\x02\\x00\\x00\\x86+\\x00\\x00\\x06\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Gomalco Productions.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x80F\\x00\\x001\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                               text  \\\n",
       "0        0                                \"Gobryas is there?\"   \n",
       "1        0                                His name's Gonzago.   \n",
       "2        0                                  Goneril, gonəril.   \n",
       "3        0  In discussing the character of Hlestakov, the ...   \n",
       "4        0                               Gomalco Productions.   \n",
       "\n",
       "                                 retriever_input_ids  \\\n",
       "0  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "1  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "2  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "3  b\"e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "4  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "\n",
       "                                 generator_input_ids  \n",
       "0  b'S\\xc4\\x00\\x00n\\x01\\x00\\x00&\\x00\\x00\\x00\\xa0\\...  \n",
       "1  b'S\\xc4\\x00\\x00_\\t\\x00\\x00\\x9e\\x05\\x00\\x00R\\x0...  \n",
       "2  b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x9b8\\x00\\x00Z\\...  \n",
       "3  b'S\\xc4\\x00\\x00*\\x02\\x00\\x00\\x86+\\x00\\x00\\x06\\...  \n",
       "4  b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x80F\\x00\\x001\\...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_guttenberg = df_sentences_guttenberg.loc[\n",
    "    (df_sentences_guttenberg[\"retriever_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_guttenberg = df_sentences_guttenberg.loc[\n",
    "    (df_sentences_guttenberg[\"generator_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_guttenberg = df_sentences_guttenberg.reset_index(drop=True)\n",
    "df_sentences_guttenberg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retriever_input_ids</th>\n",
       "      <th>generator_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Sorry, Soren.\"</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x0018\\x00\\x00\\x0b\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3421</td>\n",
       "      <td>Are they alive?</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x87\\x10\\x00\\x00\\xe4\\x01\\x00\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3420</td>\n",
       "      <td>What DO you do?</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00K\\x07\\x00\\x00\\xda \\x00\\x00Y\\x01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3419</td>\n",
       "      <td>\"This is yours.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x00\\xbc\\x04\\x00\\x00&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3418</td>\n",
       "      <td>Leave the halls</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00&amp;D\\x00\\x00\\x06\\x01\\x00\\x00\\x1e_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id             text                                retriever_input_ids  \\\n",
       "0     0  \"Sorry, Soren.\"  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "1  3421  Are they alive?  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "2  3420  What DO you do?  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "3  3419  \"This is yours.  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "4  3418  Leave the halls  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "\n",
       "                                 generator_input_ids  \n",
       "0  b'S\\xc4\\x00\\x00n\\x01\\x00\\x0018\\x00\\x00\\x0b\\x00...  \n",
       "1  b'S\\xc4\\x00\\x00\\x87\\x10\\x00\\x00\\xe4\\x01\\x00\\x0...  \n",
       "2  b'S\\xc4\\x00\\x00K\\x07\\x00\\x00\\xda \\x00\\x00Y\\x01...  \n",
       "3  b'S\\xc4\\x00\\x00n\\x01\\x00\\x00\\xbc\\x04\\x00\\x00>\\...  \n",
       "4  b'S\\xc4\\x00\\x00&D\\x00\\x00\\x06\\x01\\x00\\x00\\x1e_...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_story = df_sentences_story.loc[\n",
    "    (df_sentences_story[\"retriever_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_story = df_sentences_story.loc[\n",
    "    (df_sentences_story[\"generator_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_story = df_sentences_story.reset_index(drop=True)\n",
    "df_sentences_story.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import chain\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-5\n",
    "RETRIEVER_INPUT_MAX_LENGTH = 256\n",
    "RETRIEVER_LAST_OUTPUT_SENTENCES = 3\n",
    "KNN_N_NEIGHBOURS = 4\n",
    "MAX_RELEVANT_TOKENS = 512\n",
    "LR = 1e-4\n",
    "LM_LOSS_ABG = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(retriever, retriever_tokenizer, input_ids):\n",
    "    def _prepare_ids(ids, max_length):\n",
    "        ids = list(ids)\n",
    "        if len(ids) < max_length:\n",
    "            return ids + [retriever_tokenizer.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            return ids[:max_length]\n",
    "\n",
    "    max_length = max([len(item) for item in input_ids])\n",
    "    if max_length % 8 != 0:\n",
    "        max_length += 8 - max_length % 8\n",
    "    if max_length > RETRIEVER_INPUT_MAX_LENGTH:\n",
    "        max_length = RETRIEVER_INPUT_MAX_LENGTH\n",
    "    \n",
    "    padded_input_ids = torch.LongTensor([\n",
    "        _prepare_ids(item, max_length) for item in input_ids\n",
    "    ])\n",
    "    attention_mask = padded_input_ids != retriever_tokenizer.pad_token_id\n",
    "    hidden_state = retriever(input_ids=padded_input_ids.to(retriever.device),\n",
    "                             attention_mask=attention_mask.to(retriever.device),\n",
    "                             output_hidden_states=True).last_hidden_state\n",
    "    cls_embedding = hidden_state[:, 0, :]\n",
    "    cls_embedding_norm = torch.sqrt( (cls_embedding ** 2).sum(dim=-1, keepdims=True) ) + EPS\n",
    "    return cls_embedding / cls_embedding_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(retriever, retriever_tokenizer, inputs, batch_size, verbose=False):\n",
    "    inputs = list(inputs)\n",
    "    df_sort = pd.DataFrame({\n",
    "        \"index\": range(len(inputs)),\n",
    "        \"inputs\": inputs,\n",
    "        \"length\": [len(row) for row in inputs]\n",
    "    })\n",
    "    df_sort = df_sort.sort_values(\"length\", ascending=False)\n",
    "\n",
    "    embeddings = np.zeros([len(inputs), retriever.config.hidden_size], dtype=np.float16)\n",
    "\n",
    "    batch_count = int(np.ceil(len(inputs) / batch_size))\n",
    "    with torch.no_grad():\n",
    "        iterable = range(batch_count)\n",
    "        if verbose:\n",
    "            iterable = tqdm(iterable)\n",
    "        for i in iterable:\n",
    "            batch_df_sort = df_sort.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "            batch_input_ids = batch_df_sort[\"inputs\"].apply(decode_input_ids).tolist()\n",
    "            batch_embeddings_torch = get_batch_embeddings(retriever, retriever_tokenizer, batch_input_ids)\n",
    "            embeddings[batch_df_sort[\"index\"].tolist()] = batch_embeddings_torch.detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_prompt_embeddings(retriever, generator, retriever_tokenizer, df_sentences_guttenberg, df_sentences_story):\n",
    "    print(\"UPDATING EMBEDDINGS CACHE\")\n",
    "    retriever.eval()\n",
    "    retriever.cuda()\n",
    "    extra_embeddings = get_embeddings(retriever, retriever_tokenizer, df_sentences_guttenberg[\"retriever_input_ids\"], 64, verbose=True)\n",
    "    story_embeddings = get_embeddings(retriever, retriever_tokenizer, df_sentences_story[\"retriever_input_ids\"], 64, verbose=True)\n",
    "    return extra_embeddings, story_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrieverInput:\n",
    "    input_ids: np.ndarray\n",
    "    story_sentences: np.ndarray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StoryInputSample:\n",
    "    generator_input_ids: np.ndarray\n",
    "    generator_input_weights: np.ndarray\n",
    "    retriever_inputs: List[RetrieverInput]\n",
    "\n",
    "\n",
    "def get_rows(df_story_content, id, story_id):\n",
    "    rows = []\n",
    "    df_story_content = df_story_content.loc[df_story_content[\"story_id\"] == story_id].set_index(\"id\")\n",
    "    while id != -1:\n",
    "        try:\n",
    "            row = df_story_content.loc[[id]].iloc[0]\n",
    "        except:\n",
    "            break\n",
    "        rows.append(row)\n",
    "        id = row[\"parent_id\"]\n",
    "    return rows[::-1]\n",
    "\n",
    "\n",
    "def extract_story_inputs(rows, df_sentences_context_mapping):\n",
    "    def _get_generator_inputs(row):\n",
    "        if row[\"parent_id\"] != -1:\n",
    "            row_input = list(decode_input_ids(row[\"input_generator_input_ids\"]))\n",
    "        else:\n",
    "            row_input = []\n",
    "        row_output = list(decode_input_ids(row[\"output_generator_input_ids\"]))\n",
    "        row_content = row_input + row_output\n",
    "\n",
    "        return row_content\n",
    "\n",
    "    def _get_previous_sentences(row):\n",
    "        mask = (df_sentences_context_mapping[\"story_id\"] == row[\"story_id\"]) & \\\n",
    "               (df_sentences_context_mapping[\"context_id\"] == row[\"parent_id\"])\n",
    "        return np.array(sorted(df_sentences_context_mapping.loc[mask, \"sentence_id\"]))\n",
    "\n",
    "    def _get_retriever_input_ids(rows):\n",
    "        last_row = rows[-1]\n",
    "        input_sentences_pairs = []\n",
    "        for row in rows[-2:]:\n",
    "            if row[\"parent_id\"] != -1:\n",
    "                input_ids = decode_input_ids(row[\"input_retriever_input_ids\"])\n",
    "                story_sentence_ids = _get_previous_sentences(row)\n",
    "                input_sentences_pairs.append(RetrieverInput(input_ids, story_sentence_ids))\n",
    "        return input_sentences_pairs\n",
    "\n",
    "    generator_input_ids = []\n",
    "    generator_children_counts = []\n",
    "    for row in rows:\n",
    "        row_content = _get_generator_inputs(row)\n",
    "        if row[\"children_count\"] == 0:\n",
    "            generator_children_counts.append((len(row_content), 1))\n",
    "        else:\n",
    "            generator_children_counts.append((len(row_content), row[\"children_count\"]))\n",
    "        generator_input_ids += row_content\n",
    "    generator_weights = []\n",
    "    k = 1.0\n",
    "    for i, (token_count, children_count) in enumerate(generator_children_counts[::-1]):\n",
    "        if i > 0:\n",
    "            k *= (1 / children_count)\n",
    "        generator_weights += [k] * token_count\n",
    "    generator_weights = generator_weights[::-1]\n",
    "    retriever_inputs = _get_retriever_input_ids(rows)\n",
    "    return StoryInputSample(generator_input_ids, generator_weights, retriever_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_description_encode(df_stories, story_id, generator_tokenizer):\n",
    "    tags = \"[TAGS] \" + df_stories.loc[df_stories[\"id\"] == story_id, \"tags\"].values[0]\n",
    "    return generator_tokenizer.encode(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_input(df_story_content, row_id, story_id, df_sentences_context_mapping, retriever_tokenizer):\n",
    "    rows = get_rows(df_story_content, row_id, story_id)\n",
    "    if len(rows) == 1:\n",
    "        parent_id = rows[-1][\"parent_id\"]\n",
    "    else:\n",
    "        parent_id = rows[-2][\"parent_id\"]\n",
    "    retriever_requests = [\n",
    "        (\n",
    "            decode_input_ids(rows[-1][\"input_retriever_input_ids\"]).tolist(),\n",
    "            df_sentences_context_mapping.loc[\n",
    "                (df_sentences_context_mapping[\"story_id\"] == story_id) & \\\n",
    "                (df_sentences_context_mapping[\"context_id\"] == parent_id),\n",
    "                \"sentence_id\"\n",
    "            ].tolist()\n",
    "        )\n",
    "    ]\n",
    "    if len(rows) > 1:\n",
    "        sentences_to_search = df_sentences_context_mapping.loc[\n",
    "            (df_sentences_context_mapping[\"story_id\"] == story_id) & \\\n",
    "            (df_sentences_context_mapping[\"context_id\"] == parent_id),\n",
    "            \"sentence_id\"\n",
    "        ].tolist()\n",
    "        query_sentences = nltk.sent_tokenize(rows[-2][\"output\"])[-RETRIEVER_LAST_OUTPUT_SENTENCES:]\n",
    "        retriever_requests += [\n",
    "            (retriever_tokenizer.encode(sent), sentences_to_search)\n",
    "            for sent in query_sentences\n",
    "        ]\n",
    "    return extract_story_inputs(rows, df_sentences_context_mapping), retriever_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_distance(x, y):\n",
    "    return -(x * y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_nearest_df(retriever_request_embeddings, cached_extra_nn, cached_story_nn, df_extra, df_story):\n",
    "    retriever_request_embeddings_np = retriever_request_embeddings.detach().cpu().numpy()\n",
    "    extra_indices = []\n",
    "    extra_distances = []\n",
    "    for distances, indices in zip(*cached_extra_nn.kneighbors(retriever_request_embeddings_np)):\n",
    "        extra_indices += list(indices)\n",
    "        extra_distances += list(distances)\n",
    "    sub_df_extra = df_extra.iloc[extra_indices][[\"text\", \"retriever_input_ids\", \"generator_input_ids\"]]\n",
    "    sub_df_extra[\"distance\"] = extra_distances\n",
    "\n",
    "    if cached_story_nn is not None:\n",
    "        story_indices = []\n",
    "        story_distances = []\n",
    "        for distances, indices in zip(*cached_story_nn.kneighbors(retriever_request_embeddings_np)):\n",
    "            story_indices += list(indices)\n",
    "            story_distances += list(distances)\n",
    "        sub_df_story = df_story.iloc[story_indices][[\"text\", \"retriever_input_ids\", \"generator_input_ids\"]]\n",
    "        sub_df_story[\"distance\"] = story_distances\n",
    "    \n",
    "    if cached_story_nn is not None:\n",
    "        df = pd.concat([sub_df_extra, sub_df_story]).reset_index(drop=True)\n",
    "    else:\n",
    "        df = sub_df_extra.reset_index(drop=True)\n",
    "    df = df.sort_values(\"distance\")\n",
    "    df = df.drop_duplicates(\"text\")\n",
    "    df = df.head( (RETRIEVER_LAST_OUTPUT_SENTENCES + 1) * KNN_N_NEIGHBOURS)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_generator_input(input_ids, weights, generator, generator_tokenizer, tag_input_ids, nearest_input_ids):\n",
    "    max_story_token_count = generator.config.max_position_embeddings - len(tag_input_ids) - len(nearest_input_ids)\n",
    "    generator_input_ids = input_ids[-max_story_token_count:]\n",
    "    generator_weights = weights[-max_story_token_count:]\n",
    "\n",
    "    input_tid, = generator_tokenizer.convert_tokens_to_ids([\"[INPUT]\"])\n",
    "    output_tid, = generator_tokenizer.convert_tokens_to_ids([\"[OUTPUT]\"])\n",
    "\n",
    "    if generator_input_ids[0] not in {input_tid, output_tid}:\n",
    "        if input_tid not in generator_input_ids:\n",
    "            input_start = None\n",
    "        else:\n",
    "            input_start = list(generator_input_ids).index(input_tid)\n",
    "        if output_tid not in generator_input_ids:\n",
    "            output_start = None\n",
    "        else:\n",
    "            output_start = list(generator_input_ids).index(output_tid)\n",
    "        if output_start is not None and input_start is not None:\n",
    "            if input_start < output_start:\n",
    "                start_token = output_tid\n",
    "            else:\n",
    "                start_token = input_tid\n",
    "        elif output_start is not None:\n",
    "            start_token = input_tid\n",
    "        else:\n",
    "            start_token = output_tid\n",
    "        generator_input_ids[0] = start_token\n",
    "    \n",
    "    return generator_input_ids, generator_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_input(cached_extra_nn, row, df_story_content, df_stories):\n",
    "    generator_input, retriever_requests = story_input(df_story_content,\n",
    "                                                      row[\"id\"],\n",
    "                                                      row[\"story_id\"],\n",
    "                                                      df_sentences_context_mapping,\n",
    "                                                      retriever_tokenizer)\n",
    "    story_sentence_ids = set(chain(*[sentences for _, sentences in retriever_requests]))\n",
    "    story_sentence_mask = df_sentences_story[\"id\"].isin(story_sentence_ids)\n",
    "    story_sentence_count = story_sentence_mask.sum()\n",
    "\n",
    "    if story_sentence_count > 0:\n",
    "        cached_story_nn = NearestNeighbors(n_neighbors=min(KNN_N_NEIGHBOURS, int(story_sentence_mask.sum())),\n",
    "                                        metric=dot_distance,\n",
    "                                        n_jobs=-1,\n",
    "                                        algorithm=\"brute\")\n",
    "        cached_story_nn.fit(cached_story_embeddings[story_sentence_mask])\n",
    "    else:\n",
    "        cached_story_nn = None\n",
    "\n",
    "    retriever_request_embeddings = get_batch_embeddings(retriever, retriever_tokenizer, [\n",
    "        input_ids\n",
    "        for input_ids, _ in retriever_requests\n",
    "    ])\n",
    "    df_cached_nearest = get_cached_nearest_df(retriever_request_embeddings,\n",
    "                                              cached_extra_nn,\n",
    "                                              cached_story_nn,\n",
    "                                              df_sentences_guttenberg,\n",
    "                                              df_sentences_story.loc[story_sentence_mask])\n",
    "    retriever_cached_relevant_embeddings = get_batch_embeddings(\n",
    "        retriever,\n",
    "        retriever_tokenizer,\n",
    "        df_cached_nearest[\"retriever_input_ids\"].apply(decode_input_ids)\n",
    "    )\n",
    "    retriever_distances = -retriever_request_embeddings.matmul(retriever_cached_relevant_embeddings.T)\n",
    "\n",
    "    retriever_nearest_indices = retriever_distances.mean(dim=0).sort().indices.detach().cpu().numpy()\n",
    "    df_nearest = df_cached_nearest.iloc[retriever_nearest_indices]\n",
    "    nearest_samples_input_ids = df_nearest[\"generator_input_ids\"].apply(decode_input_ids)    \n",
    "\n",
    "    nearest_input_ids = np.array(list(chain(*nearest_samples_input_ids))[:MAX_RELEVANT_TOKENS-1])\n",
    "    nearest_weights = np.zeros([len(nearest_input_ids)])\n",
    "    \n",
    "    tags_string = df_stories.loc[df_stories[\"id\"] == row[\"story_id\"], \"tags\"].values[0]\n",
    "    if pd.isna(tags_string):\n",
    "        tags_string = \"\"\n",
    "    tag_input_ids = np.array(generator_tokenizer.encode(\"[TAGS] \" + tags_string))\n",
    "    tag_weights = np.zeros([len(tag_input_ids)])\n",
    "\n",
    "    generator_input_ids, generator_weights = cut_generator_input(\n",
    "        generator_input.generator_input_ids,\n",
    "        generator_input.generator_input_weights,\n",
    "        generator,\n",
    "        generator_tokenizer,\n",
    "        tag_input_ids,\n",
    "        nearest_input_ids\n",
    "    )\n",
    "\n",
    "    input_ids = list(tag_input_ids) + list(nearest_input_ids) + list(generator_input_ids)\n",
    "    weights = list(tag_weights) + list(nearest_weights) + list(generator_weights)\n",
    "\n",
    "    return input_ids, weights, retriever_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lm_loss(input_ids, weights, logits):\n",
    "    batch_size, _, class_count = logits.shape\n",
    "    input_ids_shifted = input_ids[:, 1:].reshape([-1])\n",
    "    logits_shifted = logits[:, :-1, :].reshape([-1, class_count])\n",
    "    tokenwise_ce = F.cross_entropy(logits_shifted,\n",
    "                                   input_ids_shifted,\n",
    "                                   reduction=\"none\")\\\n",
    "        .reshape([batch_size, -1])\n",
    "    loss_samplewise = (tokenwise_ce * weights[:, 1:]).sum(dim=-1) / (weights[:, 1:].sum(dim=-1) + EPS)\n",
    "    return loss_samplewise.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_loss(cached_extra_nn, row, df_story_content_train, df_stories_train, lm_loss_avg):\n",
    "    input_ids, weights, retriever_distances = get_nn_input(cached_extra_nn,\n",
    "                                                           row,\n",
    "                                                           df_story_content_train,\n",
    "                                                           df_stories_train)\n",
    "    generator_input_ids = torch.LongTensor([input_ids]).to(generator.device)\n",
    "    generator_weights = torch.FloatTensor([weights]).to(generator.device)\n",
    "    generator_output = generator(generator_input_ids).logits\n",
    "    lm_loss = calc_lm_loss(generator_input_ids, generator_weights, generator_output)\n",
    "    distance_loss = (lm_loss - lm_loss_avg) * (1-retriever_distances).mean()\n",
    "    return lm_loss + distance_loss, lm_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationRegularizationHook():\n",
    "    def __init__(self, lambda_, norm):\n",
    "        self.regularization = 0\n",
    "        self.lambda_ = 0\n",
    "        \n",
    "    def norm(self, vec):\n",
    "        return torch.sqrt((vec ** 2).sum(dim=-1, keepdim=True) + EPS)\n",
    "        \n",
    "    def __call__(self, module, input, output):\n",
    "        try:\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                vec = output\n",
    "            elif isinstance(output, tuple) or isinstance(output, list):\n",
    "                vec = output[0]\n",
    "            self.regularization += (self.norm(vec) * self.lambda_).mean()\n",
    "        except:\n",
    "            print(f\"ERROR ON {module.__class__}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_row(regularizer_hook, optimizer, scaler, row, df_story_content_train, df_stories_train, cached_extra_nn, lm_loss_avg, lm_loss_values):\n",
    "    regularizer_hook.regularization = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        loss, lm_loss_value = get_row_loss(cached_extra_nn, row, df_story_content_train, df_stories_train, lm_loss_avg)\n",
    "        loss += regularizer_hook.regularization\n",
    "        lm_loss_values.append(lm_loss_value)\n",
    "        lm_loss_avg = sum(lm_loss_values) / len(lm_loss_values)\n",
    "        if len(lm_loss_values) == LM_LOSS_ABG:\n",
    "            lm_loss_values = lm_loss_values[-LM_LOSS_ABG:]\n",
    "    if pd.isna(loss.item()):\n",
    "        raise ValueError(\"NAN LOSS\")\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    return loss, lm_loss_value, lm_loss_values, lm_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_row(row, df_story_content_test, df_stories_test, cached_extra_nn):\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, lm_loss_value = get_row_loss(cached_extra_nn, row, df_story_content_test, df_stories_test, lm_loss_avg)\n",
    "            \n",
    "    return loss, lm_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retriever.cuda()\n",
    "generator = generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_story_content_train = df_story_content.loc[df_story_content[\"story_id\"].isin(df_stories_train[\"id\"])]\n",
    "df_story_content_train = df_story_content_train.sample(len(df_story_content_train), random_state=42)\\\n",
    "    .reset_index(drop=True)\n",
    "    \n",
    "df_story_content_test = df_story_content.loc[df_story_content[\"story_id\"].isin(df_stories_test[\"id\"])]\n",
    "df_story_content_test = df_story_content_test.sample(len(df_story_content_test), random_state=42)\\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizable_params = list(generator.parameters()) + list(retriever.parameters())\n",
    "optimizer = SGD(optimizable_params, lr=LR)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer_hook = ActivationRegularizationHook(lambda_=0.01,\n",
    "                                                norm=2)\n",
    "generator.lm_head.register_forward_hook(regularizer_hook)\n",
    "generator.transformer.wte.register_forward_hook(regularizer_hook)\n",
    "generator.transformer.wpe.register_forward_hook(regularizer_hook)\n",
    "generator.transformer.ln_f.register_forward_hook(regularizer_hook)\n",
    "for layer in generator.transformer.h:\n",
    "    layer.register_forward_hook(regularizer_hook)\n",
    "retriever.embeddings.register_forward_hook(regularizer_hook)\n",
    "for layer in retriever.encoder.layer:\n",
    "    layer.register_forward_hook(regularizer_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_CACHE_FREQ = 500\n",
    "DEBUG_SHOW_LOSS_FREQ = 25\n",
    "EPOCH_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [03:09<00:00, 139.30it/s]\n",
      "100%|██████████| 9749/9749 [01:08<00:00, 142.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 0 STEP 46.306278228759766 LOSS 16.122961044311523 LM LOSS\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NAN LOSS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12363/333262165.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                                                  \u001b[0mcached_extra_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                                                  \u001b[0mlm_loss_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                                                  lm_loss_values)\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloss_values_mean_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mlm_loss_values_mean_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_loss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12363/1662291365.py\u001b[0m in \u001b[0;36mtrain_row\u001b[0;34m(regularizer_hook, optimizer, scaler, row, df_story_content_train, df_stories_train, cached_extra_nn, lm_loss_avg, lm_loss_values)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mlm_loss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_loss_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mLM_LOSS_ABG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NAN LOSS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: NAN LOSS"
     ]
    }
   ],
   "source": [
    "i = -1\n",
    "lm_loss_values = []\n",
    "lm_loss_avg = 0.0\n",
    "j = -1\n",
    "best_loss = np.inf\n",
    "loss_values_mean_container = []\n",
    "lm_loss_values_mean_container = []\n",
    "while True:\n",
    "    i += 1\n",
    "    j += 1\n",
    "    if i == len(df_story_content_train):\n",
    "        i = 0\n",
    "    if i % UPDATE_CACHE_FREQ == 0:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            cached_extra_embeddings, cached_story_embeddings = get_cached_prompt_embeddings(retriever,\n",
    "                                                                                            generator,\n",
    "                                                                                            retriever_tokenizer,\n",
    "                                                                                            df_sentences_guttenberg,\n",
    "                                                                                            df_sentences_story)\n",
    "            cached_extra_nn = NearestNeighbors(n_neighbors=KNN_N_NEIGHBOURS, metric=dot_distance, n_jobs=-1, algorithm=\"ball_tree\")\n",
    "            cached_extra_nn.fit(cached_extra_embeddings)\n",
    "            retriever.train()\n",
    "            generator.train()\n",
    "    row = df_story_content_train.iloc[i]\n",
    "    loss, lm_loss_value, lm_loss_values, lm_loss_avg = train_row(regularizer_hook,\n",
    "                                                                 optimizer,\n",
    "                                                                 scaler,\n",
    "                                                                 row,\n",
    "                                                                 df_story_content_train,\n",
    "                                                                 df_stories_train,\n",
    "                                                                 cached_extra_nn,\n",
    "                                                                 lm_loss_avg,\n",
    "                                                                 lm_loss_values)\n",
    "    loss_values_mean_container.append(loss.item())\n",
    "    lm_loss_values_mean_container.append(lm_loss_value)\n",
    "    if i % DEBUG_SHOW_LOSS_FREQ == 0:\n",
    "        print(f\"TRAIN {j} STEP {sum(loss_values_mean_container) / len(loss_values_mean_container)} LOSS {sum(lm_loss_values_mean_container) / len(lm_loss_values_mean_container)} LM LOSS\")\n",
    "        loss_values_mean_container = []\n",
    "        lm_loss_values_mean_container = []\n",
    "        \n",
    "    if j > 0 and j % EPOCH_SIZE == 0:\n",
    "        epoch = j // EPOCH_SIZE\n",
    "        val_loss_values = []\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                for _, row in tqdm(df_story_content_test.iterrows(), total=len(df_story_content_test)):\n",
    "                    _, lm_loss_value = validation_row(row,\n",
    "                                                 df_story_content_test,\n",
    "                                                 df_stories_test,\n",
    "                                                 cached_extra_nn)\n",
    "                    val_loss_values.append(lm_loss_value)\n",
    "        val_loss = np.array(val_loss_values).mean()\n",
    "        print(f\"VALIDATION EPOCH {epoch} LOSS {val_loss}\")\n",
    "        with open(\"train-log-base.txt\", \"a\") as log_target:\n",
    "            log_target.write(f\"VALIDATION EPOCH {epoch} LOSS {val_loss}\\n\")\n",
    "        if val_loss < best_loss:\n",
    "            retriever.save_pretrained(\"checkpoint-retriever-base\")\n",
    "            generator.save_pretrained(\"checkpoint-generator-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                          950\n",
       "parent_id                                                                   947\n",
       "input                                                          Cry and run away\n",
       "output                        You can't do this anymore.\\nYou let the tears ...\n",
       "story_id                                                                  25548\n",
       "children_count                                                                1\n",
       "input_retriever_input_ids     b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x0022\\x00\\x00\\t\\x0...\n",
       "input_generator_input_ids     b'U\\xc4\\x00\\x00\\xd8\\x1f\\x00\\x00\"\\x01\\x00\\x00!\\...\n",
       "output_generator_input_ids    b'V\\xc4\\x00\\x00\\x99\\x03\\x00\\x00\\xcc\\x01\\x00\\x0...\n",
       "output_retriever_input_ids    b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x0022\\x00\\x00\\t\\x0...\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularizer_hook.regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.105204582214355"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss_value"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "interpreter": {
   "hash": "14491e825d4673210f59c83732ae5f4b564c5c040b5b1d881a577d4827971b6d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
