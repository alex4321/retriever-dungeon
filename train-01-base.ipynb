{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 02:59:05.326437: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:05.326460: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, GPT2TokenizerFast, BertModel, GPTNeoForCausalLM\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVER_BERT_MODEL = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "GENERATOR_GPTNEO_MODEL = \"EleutherAI/gpt-neo-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tokenizer = BertTokenizerFast.from_pretrained(RETRIEVER_BERT_MODEL)\n",
    "retriever_tokenizer.add_tokens([\"[STORY]\", \"[EXTRA]\", \"[RETRIEVE]\"])\n",
    "\n",
    "generator_tokenizer = GPT2TokenizerFast.from_pretrained(GENERATOR_GPTNEO_MODEL)\n",
    "generator_tokenizer.add_tokens([\"[TAGS]\", \"[INIT]\", \"[PROMPT]\", \"[TEXT]\", \"[INPUT]\", \"[OUTPUT]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'fit_denses.3.weight', 'fit_denses.1.weight', 'fit_denses.2.bias', 'fit_denses.2.weight', 'fit_denses.0.weight', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.1.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.0.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.3.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "retriever = BertModel.from_pretrained(RETRIEVER_BERT_MODEL)\n",
    "generator = GPTNeoForCausalLM.from_pretrained(GENERATOR_GPTNEO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_tokens(tokenizer, model):\n",
    "    if len(tokenizer) % 8 != 0:\n",
    "        tokenizer.add_tokens([\n",
    "            f\"[DUMB{i}]\"\n",
    "            for i in range(8 - len(tokenizer) % 8)\n",
    "        ])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tokenizer, retriever = sync_tokens(retriever_tokenizer, retriever)\n",
    "generator_tokenizer, generator = sync_tokens(generator_tokenizer, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_apply(df, func, chunk_size, process_count):\n",
    "    chunk_count = int(np.ceil(len(df) / chunk_size))\n",
    "    return list(chain(*Parallel(n_jobs=process_count)(\n",
    "        delayed(func)(df.iloc[i * chunk_size : (i + 1) * chunk_size])\n",
    "        for i in tqdm(range(chunk_count))\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_ids(input_ids):\n",
    "    return np.array(input_ids, dtype=np.int32).tobytes()\n",
    "\n",
    "def decode_input_ids(buffer):\n",
    "    return np.frombuffer(buffer, dtype=np.int32)\n",
    "\n",
    "def apply_tokenizer(texts, tokenizer):\n",
    "    return [\n",
    "        encode_input_ids(row)\n",
    "        for row in tokenizer(list(texts))[\"input_ids\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences_guttenberg = pd.read_csv(\"data/guttenberg-sentences-sampled.csv\")\n",
    "df_sentences_story = pd.read_csv(\"data/cleaned/story-sentences.csv\")\n",
    "df_sentences_context_mapping = pd.read_csv(\"data/cleaned/story-context-sentence-mapping.csv\")\n",
    "df_story_content = pd.read_csv(\"data/cleaned/story-trees.csv\")\n",
    "df_stories_train = pd.read_csv(\"data/cleaned/stories-train.csv\")\n",
    "df_stories_test = pd.read_csv(\"data/cleaned/stories-test.csv\")\n",
    "\n",
    "id2num = {id: i for i, id in enumerate(df_story_content[\"id\"].unique())}\n",
    "df_story_content[\"id\"] = df_story_content[\"id\"].apply(id2num.get)\n",
    "df_story_content[\"parent_id\"] = df_story_content[\"parent_id\"].apply(id2num.get).fillna(-1).astype(int)\n",
    "df_sentences_context_mapping[\"context_id\"] = df_sentences_context_mapping[\"context_id\"].apply(id2num.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>story_id</th>\n",
       "      <th>children_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>The land of Kronnland is a mythical, wonderful...</td>\n",
       "      <td>12487</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Start Danny's Campaign</td>\n",
       "      <td>Danny Blaze\\nBackground :\\nBorn in the summer ...</td>\n",
       "      <td>12487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Continue</td>\n",
       "      <td>With all the townsfolk transformed into mindle...</td>\n",
       "      <td>12487</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Get back to Bren and warn him about the danger.</td>\n",
       "      <td>You run down the hill as Andrew's army regroup...</td>\n",
       "      <td>12487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Watch the battle from your hideout.</td>\n",
       "      <td>Although worried, you stay in your hideout and...</td>\n",
       "      <td>12487</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  parent_id                                            input  \\\n",
       "0   0         -1                                           [ROOT]   \n",
       "1   1          0                          Start Danny's Campaign    \n",
       "2   2          1                                         Continue   \n",
       "3   3          2  Get back to Bren and warn him about the danger.   \n",
       "4   4          2              Watch the battle from your hideout.   \n",
       "\n",
       "                                              output  story_id  children_count  \n",
       "0  The land of Kronnland is a mythical, wonderful...     12487               3  \n",
       "1  Danny Blaze\\nBackground :\\nBorn in the summer ...     12487               1  \n",
       "2  With all the townsfolk transformed into mindle...     12487               2  \n",
       "3  You run down the hill as Andrew's army regroup...     12487               0  \n",
       "4  Although worried, you stay in your hideout and...     12487               2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2children_count = df_story_content.groupby([\"story_id\", \"parent_id\"])[\"id\"].nunique().to_dict()\n",
    "df_story_content[\"children_count\"] = df_story_content[[\"story_id\", \"id\"]].apply(\n",
    "    lambda row: id2children_count.get((row[\"story_id\"], row[\"id\"]), 0),\n",
    "    axis=1\n",
    ")\n",
    "df_story_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 16/184 [00:01<00:13, 12.91it/s]2021-11-26 02:59:57.873281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:57.873477: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:57.877899: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:57.878043: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:57.921518: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:57.921666: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:57.943183: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:57.943328: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.002362: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.002544: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.021008: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.021144: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.034007: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.034318: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.057964: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.058001: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.066950: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.067081: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.099313: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.099350: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.115213: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.115474: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.179327: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.179361: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.393971: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.394210: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.428155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.428322: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.516322: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.516470: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-11-26 02:59:58.549901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 02:59:58.550345: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "100%|██████████| 184/184 [00:18<00:00,  9.97it/s]\n",
      " 70%|██████▉   | 128/184 [00:11<00:06,  8.81it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2103 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 184/184 [00:17<00:00, 10.65it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sentences_guttenberg[\"retriever_input_ids\"] = parallel_apply(\n",
    "    df_sentences_guttenberg,\n",
    "    lambda df: apply_tokenizer(\"[EXTRA] \" + df[\"text\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_sentences_guttenberg[\"generator_input_ids\"] = parallel_apply(\n",
    "    df_sentences_guttenberg,\n",
    "    lambda df: apply_tokenizer(\"[PROMPT] \" + df[\"text\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:02<00:00, 29.31it/s]\n",
      "100%|██████████| 68/68 [00:03<00:00, 22.15it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sentences_story[\"retriever_input_ids\"] = parallel_apply(\n",
    "    df_sentences_story,\n",
    "    lambda df: apply_tokenizer(\"[STORY] \" + df[\"text\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_sentences_story[\"generator_input_ids\"] = parallel_apply(\n",
    "    df_sentences_story,\n",
    "    lambda df: apply_tokenizer(\"[PROMPT] \" + df[\"text\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 8185.15it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 8637.87it/s]\n"
     ]
    }
   ],
   "source": [
    "df_story_content[\"input_retriever_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[RETRIEVE] \" + df[\"input\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_story_content[\"input_generator_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[INPUT] \" + df[\"input\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 11357.88it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2157 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2219 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2056 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2362 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2105 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3423 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2088 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 7/7 [00:00<00:00, 8224.13it/s]\n"
     ]
    }
   ],
   "source": [
    "df_story_content[\"output_generator_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[OUTPUT] \" + df[\"output\"].fillna(\"\"), generator_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")\n",
    "df_story_content[\"output_retriever_input_ids\"] = parallel_apply(\n",
    "    df_story_content,\n",
    "    lambda df: apply_tokenizer(\"[RETRIEVE] \" + df[\"output\"].fillna(\"\"), retriever_tokenizer),\n",
    "    9192,\n",
    "    -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>text</th>\n",
       "      <th>retriever_input_ids</th>\n",
       "      <th>generator_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Gobryas is there?\"</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x00&amp;\\x00\\x00\\x00\\xa0\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>His name's Gonzago.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00_\\t\\x00\\x00\\x9e\\x05\\x00\\x00R\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Goneril, gonəril.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x9b8\\x00\\x00Z\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>In discussing the character of Hlestakov, the ...</td>\n",
       "      <td>b\"e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00*\\x02\\x00\\x00\\x86+\\x00\\x00\\x06\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Gomalco Productions.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x80F\\x00\\x001\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                               text  \\\n",
       "0        0                                \"Gobryas is there?\"   \n",
       "1        0                                His name's Gonzago.   \n",
       "2        0                                  Goneril, gonəril.   \n",
       "3        0  In discussing the character of Hlestakov, the ...   \n",
       "4        0                               Gomalco Productions.   \n",
       "\n",
       "                                 retriever_input_ids  \\\n",
       "0  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "1  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "2  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "3  b\"e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "4  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00u\\x11\\x00\\x00\\t...   \n",
       "\n",
       "                                 generator_input_ids  \n",
       "0  b'S\\xc4\\x00\\x00n\\x01\\x00\\x00&\\x00\\x00\\x00\\xa0\\...  \n",
       "1  b'S\\xc4\\x00\\x00_\\t\\x00\\x00\\x9e\\x05\\x00\\x00R\\x0...  \n",
       "2  b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x9b8\\x00\\x00Z\\...  \n",
       "3  b'S\\xc4\\x00\\x00*\\x02\\x00\\x00\\x86+\\x00\\x00\\x06\\...  \n",
       "4  b'S\\xc4\\x00\\x00\\x92\\x01\\x00\\x00\\x80F\\x00\\x001\\...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_guttenberg = df_sentences_guttenberg.loc[\n",
    "    (df_sentences_guttenberg[\"retriever_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_guttenberg = df_sentences_guttenberg.loc[\n",
    "    (df_sentences_guttenberg[\"generator_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_guttenberg = df_sentences_guttenberg.reset_index(drop=True)\n",
    "df_sentences_guttenberg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retriever_input_ids</th>\n",
       "      <th>generator_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Sorry, Soren.\"</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x0018\\x00\\x00\\x0b\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3421</td>\n",
       "      <td>Are they alive?</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00\\x87\\x10\\x00\\x00\\xe4\\x01\\x00\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3420</td>\n",
       "      <td>What DO you do?</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00K\\x07\\x00\\x00\\xda \\x00\\x00Y\\x01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3419</td>\n",
       "      <td>\"This is yours.</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00n\\x01\\x00\\x00\\xbc\\x04\\x00\\x00&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3418</td>\n",
       "      <td>Leave the halls</td>\n",
       "      <td>b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...</td>\n",
       "      <td>b'S\\xc4\\x00\\x00&amp;D\\x00\\x00\\x06\\x01\\x00\\x00\\x1e_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id             text                                retriever_input_ids  \\\n",
       "0     0  \"Sorry, Soren.\"  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "1  3421  Are they alive?  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "2  3420  What DO you do?  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "3  3419  \"This is yours.  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "4  3418  Leave the halls  b'e\\x00\\x00\\x00\\x07\\x04\\x00\\x00\\xa2\\t\\x00\\x00\\...   \n",
       "\n",
       "                                 generator_input_ids  \n",
       "0  b'S\\xc4\\x00\\x00n\\x01\\x00\\x0018\\x00\\x00\\x0b\\x00...  \n",
       "1  b'S\\xc4\\x00\\x00\\x87\\x10\\x00\\x00\\xe4\\x01\\x00\\x0...  \n",
       "2  b'S\\xc4\\x00\\x00K\\x07\\x00\\x00\\xda \\x00\\x00Y\\x01...  \n",
       "3  b'S\\xc4\\x00\\x00n\\x01\\x00\\x00\\xbc\\x04\\x00\\x00>\\...  \n",
       "4  b'S\\xc4\\x00\\x00&D\\x00\\x00\\x06\\x01\\x00\\x00\\x1e_...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_story = df_sentences_story.loc[\n",
    "    (df_sentences_story[\"retriever_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_story = df_sentences_story.loc[\n",
    "    (df_sentences_story[\"generator_input_ids\"].apply(len) // 4) <= 256\n",
    "]\n",
    "df_sentences_story = df_sentences_story.reset_index(drop=True)\n",
    "df_sentences_story.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import chain\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-4\n",
    "RETRIEVER_INPUT_MAX_LENGTH = 256\n",
    "RETRIEVER_LAST_OUTPUT_SENTENCES = 3\n",
    "KNN_N_NEIGHBOURS = 4\n",
    "MAX_RELEVANT_TOKENS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(retriever, retriever_tokenizer, input_ids):\n",
    "    def _prepare_ids(ids, max_length):\n",
    "        ids = list(ids)\n",
    "        if len(ids) < max_length:\n",
    "            return ids + [retriever_tokenizer.pad_token_id] * (max_length - len(ids))\n",
    "        else:\n",
    "            return ids[:max_length]\n",
    "\n",
    "    max_length = max([len(item) for item in input_ids])\n",
    "    if max_length % 8 != 0:\n",
    "        max_length += 8 - max_length % 8\n",
    "    if max_length > RETRIEVER_INPUT_MAX_LENGTH:\n",
    "        max_length = RETRIEVER_INPUT_MAX_LENGTH\n",
    "    \n",
    "    padded_input_ids = torch.LongTensor([\n",
    "        _prepare_ids(item, max_length) for item in input_ids\n",
    "    ])\n",
    "    attention_mask = padded_input_ids != retriever_tokenizer.pad_token_id\n",
    "    hidden_state = retriever(input_ids=padded_input_ids.to(retriever.device),\n",
    "                             attention_mask=attention_mask.to(retriever.device),\n",
    "                             output_hidden_states=True).last_hidden_state\n",
    "    cls_embedding = hidden_state[:, 0, :]\n",
    "    cls_embedding_norm = torch.sqrt( (cls_embedding ** 2).sum(dim=-1, keepdims=True) ) + EPS\n",
    "    return cls_embedding / cls_embedding_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(retriever, retriever_tokenizer, inputs, batch_size, verbose=False):\n",
    "    inputs = list(inputs)\n",
    "    df_sort = pd.DataFrame({\n",
    "        \"index\": range(len(inputs)),\n",
    "        \"inputs\": inputs,\n",
    "        \"length\": [len(row) for row in inputs]\n",
    "    })\n",
    "    df_sort = df_sort.sort_values(\"length\", ascending=False)\n",
    "\n",
    "    embeddings = np.zeros([len(inputs), retriever.config.hidden_size], dtype=np.float16)\n",
    "\n",
    "    batch_count = int(np.ceil(len(inputs) / batch_size))\n",
    "    with torch.no_grad():\n",
    "        iterable = range(batch_count)\n",
    "        if verbose:\n",
    "            iterable = tqdm(iterable)\n",
    "        for i in iterable:\n",
    "            batch_df_sort = df_sort.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "            batch_input_ids = batch_df_sort[\"inputs\"].apply(decode_input_ids).tolist()\n",
    "            batch_embeddings_torch = get_batch_embeddings(retriever, retriever_tokenizer, batch_input_ids)\n",
    "            embeddings[batch_df_sort[\"index\"].tolist()] = batch_embeddings_torch.detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_prompt_embeddings(retriever, generator, retriever_tokenizer, df_sentences_guttenberg, df_sentences_story):\n",
    "    print(\"UPDATING EMBEDDINGS CACHE\")\n",
    "    retriever.eval()\n",
    "    retriever.cuda()\n",
    "    extra_embeddings = get_embeddings(retriever, retriever_tokenizer, df_sentences_guttenberg[\"retriever_input_ids\"], 64, verbose=True)\n",
    "    story_embeddings = get_embeddings(retriever, retriever_tokenizer, df_sentences_story[\"retriever_input_ids\"], 64, verbose=True)\n",
    "    return extra_embeddings, story_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrieverInput:\n",
    "    input_ids: np.ndarray\n",
    "    story_sentences: np.ndarray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StoryInputSample:\n",
    "    generator_input_ids: np.ndarray\n",
    "    generator_input_weights: np.ndarray\n",
    "    retriever_inputs: List[RetrieverInput]\n",
    "\n",
    "\n",
    "def get_rows(df_story_content, id, story_id):\n",
    "    rows = []\n",
    "    df_story_content = df_story_content.loc[df_story_content[\"story_id\"] == story_id].set_index(\"id\")\n",
    "    while id != -1:\n",
    "        try:\n",
    "            row = df_story_content.loc[[id]].iloc[0]\n",
    "        except:\n",
    "            break\n",
    "        rows.append(row)\n",
    "        id = row[\"parent_id\"]\n",
    "    return rows[::-1]\n",
    "\n",
    "\n",
    "def extract_story_inputs(rows, df_sentences_context_mapping):\n",
    "    def _get_generator_inputs(row):\n",
    "        if row[\"parent_id\"] != -1:\n",
    "            row_input = list(decode_input_ids(row[\"input_generator_input_ids\"]))\n",
    "        else:\n",
    "            row_input = []\n",
    "        row_output = list(decode_input_ids(row[\"output_generator_input_ids\"]))\n",
    "        row_content = row_input + row_output\n",
    "\n",
    "        return row_content\n",
    "\n",
    "    def _get_previous_sentences(row):\n",
    "        mask = (df_sentences_context_mapping[\"story_id\"] == row[\"story_id\"]) & \\\n",
    "               (df_sentences_context_mapping[\"context_id\"] == row[\"parent_id\"])\n",
    "        return np.array(sorted(df_sentences_context_mapping.loc[mask, \"sentence_id\"]))\n",
    "\n",
    "    def _get_retriever_input_ids(rows):\n",
    "        last_row = rows[-1]\n",
    "        input_sentences_pairs = []\n",
    "        for row in rows[-2:]:\n",
    "            if row[\"parent_id\"] != -1:\n",
    "                input_ids = decode_input_ids(row[\"input_retriever_input_ids\"])\n",
    "                story_sentence_ids = _get_previous_sentences(row)\n",
    "                input_sentences_pairs.append(RetrieverInput(input_ids, story_sentence_ids))\n",
    "        return input_sentences_pairs\n",
    "\n",
    "    generator_input_ids = []\n",
    "    generator_children_counts = []\n",
    "    for row in rows:\n",
    "        row_content = _get_generator_inputs(row)\n",
    "        if row[\"children_count\"] == 0:\n",
    "            generator_children_counts.append((len(row_content), 1))\n",
    "        else:\n",
    "            generator_children_counts.append((len(row_content), row[\"children_count\"]))\n",
    "        generator_input_ids += row_content\n",
    "    generator_weights = []\n",
    "    k = 1.0\n",
    "    for token_count, children_count in generator_children_counts[::-1]:\n",
    "        k *= (1 / children_count)\n",
    "        generator_weights += [k] * token_count\n",
    "    generator_weights = generator_weights[::-1]\n",
    "    retriever_inputs = _get_retriever_input_ids(rows)\n",
    "    return StoryInputSample(generator_input_ids, generator_weights, retriever_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_description_encode(df_stories, story_id, generator_tokenizer):\n",
    "    tags = \"[TAGS] \" + df_stories.loc[df_stories[\"id\"] == story_id, \"tags\"].values[0]\n",
    "    return generator_tokenizer.encode(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_input(df_story_content, row_id, story_id, df_sentences_context_mapping, retriever_tokenizer):\n",
    "    rows = get_rows(df_story_content, row_id, story_id)\n",
    "    if len(rows) == 1:\n",
    "        parent_id = rows[-1][\"parent_id\"]\n",
    "    else:\n",
    "        parent_id = rows[-2][\"parent_id\"]\n",
    "    retriever_requests = [\n",
    "        (\n",
    "            decode_input_ids(rows[-1][\"input_retriever_input_ids\"]).tolist(),\n",
    "            df_sentences_context_mapping.loc[\n",
    "                (df_sentences_context_mapping[\"story_id\"] == story_id) & \\\n",
    "                (df_sentences_context_mapping[\"context_id\"] == parent_id),\n",
    "                \"sentence_id\"\n",
    "            ].tolist()\n",
    "        )\n",
    "    ]\n",
    "    if len(rows) > 1:\n",
    "        sentences_to_search = df_sentences_context_mapping.loc[\n",
    "            (df_sentences_context_mapping[\"story_id\"] == story_id) & \\\n",
    "            (df_sentences_context_mapping[\"context_id\"] == parent_id),\n",
    "            \"sentence_id\"\n",
    "        ].tolist()\n",
    "        query_sentences = nltk.sent_tokenize(rows[-2][\"output\"])[-RETRIEVER_LAST_OUTPUT_SENTENCES:]\n",
    "        retriever_requests += [\n",
    "            (retriever_tokenizer.encode(sent), sentences_to_search)\n",
    "            for sent in query_sentences\n",
    "        ]\n",
    "    return extract_story_inputs(rows, df_sentences_context_mapping), retriever_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_distance(x, y):\n",
    "    return -(x * y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_nearest_df(retriever_request_embeddings, cached_extra_nn, cached_story_nn, df_extra, df_story):\n",
    "    retriever_request_embeddings_np = retriever_request_embeddings.detach().cpu().numpy()\n",
    "    extra_indices = []\n",
    "    extra_distances = []\n",
    "    for distances, indices in zip(*cached_extra_nn.kneighbors(retriever_request_embeddings_np)):\n",
    "        extra_indices += list(indices)\n",
    "        extra_distances += list(distances)\n",
    "    sub_df_extra = df_extra.iloc[extra_indices][[\"text\", \"retriever_input_ids\", \"generator_input_ids\"]]\n",
    "    sub_df_extra[\"distance\"] = extra_distances\n",
    "\n",
    "    if cached_story_nn is not None:\n",
    "        story_indices = []\n",
    "        story_distances = []\n",
    "        for distances, indices in zip(*cached_story_nn.kneighbors(retriever_request_embeddings_np)):\n",
    "            story_indices += list(indices)\n",
    "            story_distances += list(distances)\n",
    "        sub_df_story = df_story.iloc[story_indices][[\"text\", \"retriever_input_ids\", \"generator_input_ids\"]]\n",
    "        sub_df_story[\"distance\"] = story_distances\n",
    "    \n",
    "    if cached_story_nn is not None:\n",
    "        df = pd.concat([sub_df_extra, sub_df_story]).reset_index(drop=True)\n",
    "    else:\n",
    "        df = sub_df_extra.reset_index(drop=True)\n",
    "    df = df.sort_values(\"distance\")\n",
    "    df = df.drop_duplicates(\"text\")\n",
    "    df = df.head( (RETRIEVER_LAST_OUTPUT_SENTENCES + 1) * KNN_N_NEIGHBOURS)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_generator_input(input_ids, weights, generator, generator_tokenizer, tag_input_ids, nearest_input_ids):\n",
    "    max_story_token_count = generator.config.max_position_embeddings - len(tag_input_ids) - len(nearest_input_ids)\n",
    "    generator_input_ids = input_ids[-max_story_token_count:]\n",
    "    generator_weights = weights[-max_story_token_count:]\n",
    "\n",
    "    input_tid, = generator_tokenizer.convert_tokens_to_ids([\"[INPUT]\"])\n",
    "    output_tid, = generator_tokenizer.convert_tokens_to_ids([\"[OUTPUT]\"])\n",
    "\n",
    "    if generator_input_ids[0] not in {input_tid, output_tid}:\n",
    "        if input_tid not in generator_input_ids:\n",
    "            input_start = None\n",
    "        else:\n",
    "            input_start = list(generator_input_ids).index(input_tid)\n",
    "        if output_tid not in generator_input_ids:\n",
    "            output_start = None\n",
    "        else:\n",
    "            output_start = list(generator_input_ids).index(output_tid)\n",
    "        if output_start is not None and input_start is not None:\n",
    "            if input_start < output_start:\n",
    "                start_token = output_tid\n",
    "            else:\n",
    "                start_token = input_tid\n",
    "        elif output_start is not None:\n",
    "            start_token = input_tid\n",
    "        else:\n",
    "            start_token = output_tid\n",
    "        generator_input_ids[0] = start_token\n",
    "\n",
    "    return generator_input_ids, generator_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_input(cached_extra_nn, row, df_story_content, df_stories):\n",
    "    generator_input, retriever_requests = story_input(df_story_content,\n",
    "                                                      row[\"id\"],\n",
    "                                                      row[\"story_id\"],\n",
    "                                                      df_sentences_context_mapping,\n",
    "                                                      retriever_tokenizer)\n",
    "    story_sentence_ids = set(chain(*[sentences for _, sentences in retriever_requests]))\n",
    "    story_sentence_mask = df_sentences_story[\"id\"].isin(story_sentence_ids)\n",
    "    story_sentence_count = story_sentence_mask.sum()\n",
    "\n",
    "    if story_sentence_count > 0:\n",
    "        cached_story_nn = NearestNeighbors(n_neighbors=min(KNN_N_NEIGHBOURS, int(story_sentence_mask.sum())),\n",
    "                                        metric=dot_distance,\n",
    "                                        n_jobs=-1,\n",
    "                                        algorithm=\"brute\")\n",
    "        cached_story_nn.fit(cached_story_embeddings[story_sentence_mask])\n",
    "    else:\n",
    "        cached_story_nn = None\n",
    "\n",
    "    retriever_request_embeddings = get_batch_embeddings(retriever, retriever_tokenizer, [\n",
    "        input_ids\n",
    "        for input_ids, _ in retriever_requests\n",
    "    ])\n",
    "    df_cached_nearest = get_cached_nearest_df(retriever_request_embeddings,\n",
    "                                              cached_extra_nn,\n",
    "                                              cached_story_nn,\n",
    "                                              df_sentences_guttenberg,\n",
    "                                              df_sentences_story.loc[story_sentence_mask])\n",
    "    retriever_cached_relevant_embeddings = get_batch_embeddings(\n",
    "        retriever,\n",
    "        retriever_tokenizer,\n",
    "        df_cached_nearest[\"retriever_input_ids\"].apply(decode_input_ids)\n",
    "    )\n",
    "    retriever_distances = -retriever_request_embeddings.matmul(retriever_cached_relevant_embeddings.T)\n",
    "\n",
    "    retriever_nearest_indices = retriever_distances.mean(dim=0).sort().indices.detach().cpu().numpy()\n",
    "    df_nearest = df_cached_nearest.iloc[retriever_nearest_indices]\n",
    "    nearest_samples_input_ids = df_nearest[\"generator_input_ids\"].apply(decode_input_ids)    \n",
    "\n",
    "    nearest_input_ids = np.array(list(chain(*nearest_samples_input_ids))[:MAX_RELEVANT_TOKENS-1])\n",
    "    nearest_weights = np.zeros([len(nearest_input_ids)])\n",
    "    \n",
    "    tags_string = df_stories.loc[df_stories[\"id\"] == row[\"story_id\"], \"tags\"].values[0]\n",
    "    if pd.isna(tags_string):\n",
    "        tags_string = \"\"\n",
    "    tag_input_ids = np.array(generator_tokenizer.encode(\"[TAGS] \" + tags_string))\n",
    "    tag_weights = np.zeros([len(tag_input_ids)])\n",
    "\n",
    "    generator_input_ids, generator_weights = cut_generator_input(\n",
    "        generator_input.generator_input_ids,\n",
    "        generator_input.generator_input_weights,\n",
    "        generator,\n",
    "        generator_tokenizer,\n",
    "        tag_input_ids,\n",
    "        nearest_input_ids\n",
    "    )\n",
    "\n",
    "    input_ids = list(tag_input_ids) + list(nearest_input_ids) + list(generator_input_ids)\n",
    "    weights = list(tag_weights) + list(nearest_weights) + list(generator_weights)\n",
    "\n",
    "    return input_ids, weights, retriever_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retriever.cuda()\n",
    "#generator = generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_story_content_train = df_story_content.loc[df_story_content[\"story_id\"].isin(df_stories_train[\"id\"])]\n",
    "df_story_content_train = df_story_content_train.sample(len(df_story_content_train), random_state=42)\\\n",
    "    .reset_index(drop=True)\n",
    "    \n",
    "df_story_content_test = df_story_content.loc[df_story_content[\"story_id\"].isin(df_stories_test[\"id\"])]\n",
    "df_story_content_test = df_story_content_test.sample(len(df_story_content_test), random_state=42)\\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:59<00:00, 221.12it/s]\n",
      "100%|██████████| 9749/9749 [00:36<00:00, 265.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [02:01<00:00, 216.42it/s]\n",
      "100%|██████████| 9749/9749 [00:37<00:00, 260.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [02:03<00:00, 213.68it/s]\n",
      "100%|██████████| 9749/9749 [00:37<00:00, 261.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [02:12<00:00, 198.94it/s]\n",
      "100%|██████████| 9749/9749 [00:38<00:00, 251.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [02:01<00:00, 216.69it/s]\n",
      "100%|██████████| 9749/9749 [00:36<00:00, 267.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [02:15<00:00, 193.96it/s]\n",
      "100%|██████████| 9749/9749 [00:40<00:00, 241.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [02:15<00:00, 193.92it/s]\n",
      "100%|██████████| 9749/9749 [00:42<00:00, 227.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [02:18<00:00, 190.71it/s]\n",
      "100%|██████████| 9749/9749 [00:35<00:00, 275.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.19it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.45it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.32it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.65it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.93it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:47<00:00, 244.42it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.23it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.48it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.08it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.77it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.28it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.51it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.63it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.25it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.98it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:47<00:00, 244.15it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.51it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.13it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.57it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.55it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 244.06it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 244.07it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 298.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.65it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.01it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.46it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:47<00:00, 244.77it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.57it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.23it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.39it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.61it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.94it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.44it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.63it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.88it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.28it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.32it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 303.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.11it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.18it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 241.94it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.92it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.10it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.49it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.28it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.71it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.75it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.74it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 303.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.69it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 242.65it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.25it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.86it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.30it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.42it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 299.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.88it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.21it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.65it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 301.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.48it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 302.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING EMBEDDINGS CACHE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26361/26361 [01:48<00:00, 243.17it/s]\n",
      "100%|██████████| 9749/9749 [00:32<00:00, 300.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1071304/1722307125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_story_content_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         input_ids, weights, retriever_distances = get_nn_input(cached_extra_nn,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                                             \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                                             \u001b[0mdf_story_content_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1071304/1842728558.py\u001b[0m in \u001b[0;36mget_nn_input\u001b[0;34m(cached_extra_nn, row, df_story_content, df_stories)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_nn_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_extra_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_story_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     generator_input, retriever_requests = story_input(df_story_content,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                       \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                       \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"story_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                       \u001b[0mdf_sentences_context_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1071304/3749517936.py\u001b[0m in \u001b[0;36mstory_input\u001b[0;34m(df_story_content, row_id, story_id, df_sentences_context_mapping, retriever_tokenizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         ]\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mextract_story_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sentences_context_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretriever_requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1071304/3481296966.py\u001b[0m in \u001b[0;36mextract_story_inputs\u001b[0;34m(rows, df_sentences_context_mapping)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mgenerator_weights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtoken_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mgenerator_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mretriever_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_retriever_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStoryInputSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretriever_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1071304/3481296966.py\u001b[0m in \u001b[0;36m_get_retriever_input_ids\u001b[0;34m(rows)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parent_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_retriever_input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mstory_sentence_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_previous_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0minput_sentences_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRetrieverInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstory_sentence_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_sentences_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1071304/3481296966.py\u001b[0m in \u001b[0;36m_get_previous_sentences\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_previous_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_sentences_context_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"story_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"story_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                \u001b[0;34m(\u001b[0m\u001b[0mdf_sentences_context_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parent_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sentences_context_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentence_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg-transformer/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg-transformer/lib/python3.8/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__and__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__and__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__and__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logical_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mand_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rand__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg-transformer/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_logical_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5511\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5513\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg-transformer/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mlogical_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mfiller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_int\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_self_int_dtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_other_int_dtype\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfill_bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_logical_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;31m# error: Cannot call function of unknown type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg-transformer/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mna_logical_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Then Cases where this goes through without raising include:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m#  (xint or xbool) and (yint or bool)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = -1\n",
    "while True:\n",
    "    i += 1\n",
    "    if i == len(df_story_content_train):\n",
    "        i = 0\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        if i % 500 == 0:\n",
    "                cached_extra_embeddings, cached_story_embeddings = get_cached_prompt_embeddings(retriever,\n",
    "                                                                                                generator,\n",
    "                                                                                                retriever_tokenizer,\n",
    "                                                                                                df_sentences_guttenberg,\n",
    "                                                                                                df_sentences_story)\n",
    "                cached_extra_nn = NearestNeighbors(n_neighbors=KNN_N_NEIGHBOURS, metric=dot_distance, n_jobs=-1, algorithm=\"ball_tree\")\n",
    "                cached_extra_nn.fit(cached_extra_embeddings)\n",
    "                retriever.train()\n",
    "                generator.train()    \n",
    "        row = df_story_content_train.iloc[i]\n",
    "        input_ids, weights, retriever_distances = get_nn_input(cached_extra_nn,\n",
    "                                                            row,\n",
    "                                                            df_story_content_train,\n",
    "                                                            df_stories_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14491e825d4673210f59c83732ae5f4b564c5c040b5b1d881a577d4827971b6d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('kg-transformer': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
